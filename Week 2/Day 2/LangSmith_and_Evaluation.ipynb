{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "‚úãBREAKOUT ROOM #2:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: LCEL RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "- Task 6: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "1f43fdba-f1b4-4cf4-b8b0-3469edc29c80"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere lxml -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "927fd78b-8510-4bea-9e68-a3c74d3eb5a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923xinz42sWV"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m9U0SbQN2sWc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #2: Create a Simple RAG Application Using Qdrant, Hugging Face, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and Hugging Face to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs - which will serve as our data for today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `SitemapLoader` to load our PDF directly from the web!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHA9L3Jxo3r",
        "outputId": "c3535941-ce13-4ddd-ef08-b6b4ab1e507b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Fetching pages: 100%|##########| 220/220 [00:19<00:00, 11.47it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are posts from the langchain blog \n",
        "\n",
        "![Langchain Posts](images/langchain_posts.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='\n",
            "\n",
            "\n",
            "How Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "How Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\n",
            "See how Podium tests across the lifecycle development of their AI employee agent, using LangSmith for dataset curation and finetuning. They improved agent F1 response quality to 98% and reduced the need for engineering intervention by 90%.\n",
            "\n",
            "5 min read\n",
            "Aug 15, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "About PodiumPodium is a communication platform that helps small businesses connect quickly with customers via phone, text, email, and social media. Small businesses often have high-touch interactions with customers ‚Äî think automotive dealers, jewelers, bike shops ‚Äî yet are understaffed. Podium's mission is to help these businesses respond to customer inquiries promptly so that they can convert leads into sales.¬†Podium data shows that responding to customer inquiries within 5 minutes results in a 46% higher lead conversion rate than responding in an hour. To improve lead capture, Podium launched AI Employee, their agentic application (and flagship product) to engage local business customers, schedule appointments, and close sales.¬†Initially, Podium used the LangChain framework for single-turn interactions. As their agentic use cases grew more complex for a wide-ranging set of customers and domains, Podium needed better visibility into their LLM calls and interactions ‚Äî and turned to LangSmith for LLM testing and observability.Testing across the agentic development lifecycleEstablishing feedback loops was especially important to the agentic development lifecycle for Podium. LangSmith allowed the Podium engineers to test and continuously monitor their AI employee‚Äôs performance, adding new edge cases to their dataset to refine and test the model over time.Podium‚Äôs testing approach looks like the following:¬†¬†Baseline Dataset Curation: Create an initial dataset to represent basic use cases and requirements for the agent. This serves as a foundation for testing and development.Baseline Offline Evaluation: Conduct initial tests using the curated dataset to assess the agent's performance against the basic requirements before shipping to production.Collecting Feedback:¬†User-Provided Feedback: Collect direct input from users interacting with the agent.¬†Online Evaluation: Use LLMs to self-evaluate and monitor the quality of responses using in real-time, flagging potential issues for further investigation.Optimization:¬†Prompt Tuning: Refine the prompts used to guide the agent's responses.Retrieval Tuning: Adjust the retrieval mechanisms used to generate responses.Model Fine-Tuning: Use traced data to further train and specialize the model for specific tasks.Ongoing Evaluation:¬†Offline Evaluation: Evaluate the agent's performance and identify opportunities for optimization using backtesting, pairwise comparisons, and other testing methods.Dataset Curation: Continuously update and expand the test dataset with new scenarios and edge cases for regression testing, ensuring new changes don't negatively impact existing capabilities.How Podium creates testing loops for their agent Dataset curation and fine-tuning agents with LangSmithPrior to LangSmith, understanding a customer inquiry and what steps employees should take to resolve the inquiry was difficult, since the Podium engineers made 20-30 LLM calls per interaction. With LangSmith, they quickly got set up and logged and viewed traces to aggregate insights.One specific challenge Podium ran into with their AI Employee was that the agent struggled to recognize when a conversation had naturally ended, resulting in awkward repeated goodbyes. To address this, Podium began by creating a dataset in LangSmith with various conversation scenarios, including ways different conversations might conclude.¬†Their engineering team then found it helpful to upgrade to a larger model, curating the outputs into a smaller model (using a technique called model distillation). Upgrading their model went smoothly since model inputs and outputs were automatically captured in LangSmith‚Äôs traces, allowing the team to easily curate datasets.Podium engineers also enriched LangSmith traces with metadata on customer profiles, business types, and other parameters important to their business. They grouped traces using specific identifiers in LangSmith, making it easy to aggregate related traces during data curation. This enriched data enabled Podium to create a higher-quality and balanced dataset, which improved model fine-tuning and helped them avoid overfitting).¬†With this balanced dataset, the Podium team then compared the results from their fine-tuned model against results from their original, larger model using pairwise evaluations. This comparison allowed them to assess how well the upgraded model could improve the agent‚Äôs ability to know when to conclude a conversation.After fine-tuning, Podium‚Äôs new model showed significant improvement in detecting where natural conversation should end for its agent. Podium‚Äôs F1 scores with the fine-tune model experienced a 7.5% improvement, going from 91.7% to 98.6% to exceed their quality threshold of 98%.High-quality customer support for AI platform without engineering interventionAt Podium, engineers must understand when communications with customers go awry, so that they can keep shipping reliable and high-quality products.Since publicly launching their AI Employee in January, it became critical for the Technical Product Specialists (TPS) at Podium to troubleshoot issues users were encountering in real-time. At Podium, the TPS team typically provides customer support for their small business customers. However, pinpointing the source of issues (and how to take action on them) was challenging.¬†Giving the TPS team access to LangSmith provided clarity, allowing the team to quickly identify customer-reported issues and determine: ‚ÄúIs this issue caused by a bug in the application, incomplete context, misaligned instructions, or an issue with the LLM?‚Äù¬†For Podium, identifying the type of customer issue guided them to the appropriate interventions:For bugs in the application: These are orchestration failures, such as an integration failing to return data. These require engineering intervention.For incomplete context: LLM is missing information needed to answer a question. These can be remediated by the TPS team by adding additional content.For misaligned instructions: Instructions are based on business requirements; any issues in the requirements can affect agent behavior. These can be remediated by the TPS team making changes in the content authoring system to better suit business requirements.For an LLM issue: Even with necessary context, an LLM may produce unexpected or incorrect information. These require engineering intervention.For example, many car dealerships use Podium‚Äôs AI Employee to respond to customer inquiries. If the AI Employee mistakenly responds that a car dealership does not offer oil changes, the TPS team can use LangSmith‚Äôs playground feature to edit the system output and determine if a simple setting change in the Admin interface can resolve the issue.LangSmith Playground enables Podium‚Äôs support team to troubleshoot agent behavior without engineering interventionBefore LangSmith, troubleshooting agent behavior often required engineering intervention. This was a time-consuming process that involved calling in engineers to first review model inputs and outputs, and then rewrite and refactor the code.By giving their TPS team access to LangSmith traces, Podium has reduced the need for engineering intervention by 90%, allowing their engineers to focus more on development instead of support tasks.In summary, using LangSmith led to:¬†Increased efficiency of Podium‚Äôs support team by enabling them to resolve issues more quickly and independently.Improved customer satisfaction (CSAT) scores for both support interactions and Podium‚Äôs AI-powered services.What‚Äôs Next for PodiumBy integrating LangSmith and LangChain, Podium has gained a competitive edge in the space of customer experience tools. LangSmith has enhanced observability and simplified the management of large datasets and optimizing model performance. The Podium team has also been integrating LangGraph into its workflow, reducing complexity in their agent orchestration while serving different target customers, while increasing controllability over their agent conversations.¬†Together, these suite of products have allowed Podium to focus on their core value proposition ‚Äî help small businesses capture leads more effectively ‚Äî¬†and efficiently design, test, and monitor their LLM applications.Podium is hiring across roles to help local businesses win. Inspired by Podium‚Äôs story? You can also try out LangSmith for free or talk to a LangSmith expert to learn more.¬†And for a more comprehensive best practices for testing and evaluating your LLM application, check out this guidebook. \n",
            "\n",
            "\n",
            "Join our newsletter\n",
            "Updates from the LangChain team and community\n",
            "\n",
            "\n",
            "Enter your email\n",
            "\n",
            "Subscribe\n",
            "\n",
            "Processing your application...\n",
            "Success! Please check your inbox and click the link to confirm your subscription.\n",
            "Sorry, something went wrong. Please try again.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign up\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            ¬© LangChain Blog 2024\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "' metadata={'source': 'https://blog.langchain.dev/customers-podium/', 'loc': 'https://blog.langchain.dev/customers-podium/', 'lastmod': '2024-08-15T14:00:05.000Z'}\n"
          ]
        }
      ],
      "source": [
        "# lets look at what a document looks like\n",
        "print(documents[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~500 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 40,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "a666d551-6b20-4761-82ad-6f9b524d1660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 5087\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of chunks: {len(split_chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 5087 ~500 token long chunks.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "9813bfb9-cf80-4787-c962-69a5b1ccdff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max chunk length: 499\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, len(chunk.page_content))\n",
        "\n",
        "print(f\"max chunk length: {max_chunk_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=split_chunks,\n",
        "    embedding=embedding_model,\n",
        "    location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "<div style=\"border: 2px solid white; background: black; padding: 10px;\">\n",
        "\n",
        "#### üèóÔ∏è Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "You are a helpful assistant.\n",
        "Use the context provided below to answer the question asked below. \n",
        "Only use the information in the context.\n",
        "Do not use information from other sources or documents or websites.\n",
        "If you do not know the answer based on the information in the context respond with 'I have insufficient information to answer that'.\n",
        "Be clear with your answers and concise.\n",
        "Ensure your answers are correct.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2tNXIT1iuB"
      },
      "source": [
        "We'll set our Generator - `gpt-4o-min` in this case - below - to ensure we do not have rate limiting and we have reduced costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rZ-9gF1x1iEz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    #\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    #\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    #\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    #\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    #\n",
        "    | {\"response\": base_rag_prompt | base_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "4235f7af-4430-4c08-d94d-99e482ef30af"
      },
      "outputs": [],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "eb34fc03-7052-4825-82e1-85489ac84e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    **            **               \n",
            "                  **                **             \n",
            "                **                    **           \n",
            "         +--------+                     **         \n",
            "         | Lambda |                      *         \n",
            "         +--------+                      *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "  +----------------------+          +--------+     \n",
            "  | VectorStoreRetriever |          | Lambda |     \n",
            "  +----------------------+          +--------+     \n",
            "                    **            **               \n",
            "                      **        **                 \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<context,question>Output |     \n",
            "          +----------------------------------+     \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "              +------------------------+           \n",
            "              | Parallel<context>Input |           \n",
            "              +------------------------+           \n",
            "                     ***        ***                \n",
            "                    *              *               \n",
            "                  **                **             \n",
            "           +--------+          +-------------+     \n",
            "           | Lambda |          | Passthrough |     \n",
            "           +--------+          +-------------+     \n",
            "                     ***        ***                \n",
            "                        *      *                   \n",
            "                         **  **                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **              ***             \n",
            "                ***                   **           \n",
            "              **                        ***        \n",
            "+--------------------+                     **      \n",
            "| ChatPromptTemplate |                      *      \n",
            "+--------------------+                      *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "    +------------+                     +--------+  \n",
            "    | ChatOpenAI |                     | Lambda |  \n",
            "    +------------+*                  **+--------+  \n",
            "                   **              **              \n",
            "                     ***        ***                \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<response,context>Output |     \n",
            "          +----------------------------------+     \n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What's new in LangChain v0.2?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "e1633e4f-5405-46c3-f344-ffc324ac69d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain v0.2 introduces several improvements, including:\n",
            "- A full separation of langchain and langchain-community.\n",
            "- New and versioned documentation.\n",
            "- A more mature and controllable agent framework.\n",
            "- Improved LLM interface standardization, particularly regarding tool calling.\n"
          ]
        }
      ],
      "source": [
        "print(response[\"response\"].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FSZFdCM5LFoq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of found context: 4\n",
            "Context:\n",
            "page_content='Four months ago, we released the first stable version of LangChain. Today, we are following up by announcing a pre-release of langchain v0.2.This release builds upon the foundation laid in v0.1 and incorporates community feedback. We‚Äôre excited to share that v0.2 brings:¬†A much-desired full separation of langchain and langchain-community¬†New (and versioned!) docs¬†A more mature and controllable agent framework¬†Improved LLM interface standardization, particularly around tool callingBetter' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': 'e95bbca0f3d840af972479acb7c7bc2b', '_collection_name': '25a427116bbf455bb852f7f34433807e'}\n",
            "----\n",
            "Context:\n",
            "page_content='LangChain v0.2: A Leap Towards Stability\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.2: A Leap Towards Stability\n",
            "Today, we're announcing the pre-release of LangChain v0.2, which improves the stability and security of LangChain.\n",
            "\n",
            "5 min read\n",
            "May 10, 2024' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': 'ec060d963f8c46768ab57706766a844d', '_collection_name': '25a427116bbf455bb852f7f34433807e'}\n",
            "----\n",
            "Context:\n",
            "page_content='LangChain v0.1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.1.0\n",
            "\n",
            "By LangChain\n",
            "10 min read\n",
            "Jan 8, 2024' metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'loc': 'https://blog.langchain.dev/langchain-v0-1-0/', 'lastmod': '2024-02-09T21:45:58.000Z', '_id': '7864dfc50f854de28d4c7723f43255ae', '_collection_name': '25a427116bbf455bb852f7f34433807e'}\n",
            "----\n",
            "Context:\n",
            "page_content='which just released a 0.1 version. Now that langchain-core is 0.1, any future breaking changes will be accompanied by a minor version bump. This creates stability for these core abstractions which will allow others to have confidence in building on top of them.LangChain CommunityA huge part of LangChain is our partners. We have nearly 700 integrations, from document loaders to LLMs to vectorstores to toolkits. In all cases, we have strived to make it as easy to add integrations as possible, and' metadata={'source': 'https://blog.langchain.dev/the-new-langchain-architecture-langchain-core-v0-1-langchain-community-and-a-path-to-langchain-v0-1/', 'loc': 'https://blog.langchain.dev/the-new-langchain-architecture-langchain-core-v0-1-langchain-community-and-a-path-to-langchain-v0-1/', 'lastmod': '2023-12-12T20:38:14.000Z', '_id': '9f60023e9ba7477dafdc40ae6d92b060', '_collection_name': '25a427116bbf455bb852f7f34433807e'}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of found context: {len(response['context'])}\")\n",
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "1478d061-7129-4b14-c717-c99d130a9488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I have insufficient information to answer that.\n"
          ]
        }
      ],
      "source": [
        "print(response[\"response\"].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your langsmith key is: LangSmith - 5d73bab4\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\"\n",
        "print(f\"Your langsmith key is: {os.environ['LANGCHAIN_PROJECT']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "5560a787-8114-4c72-b95a-6a776043427a"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eoqBtBQERXP",
        "outputId": "fc1ea857-6784-4e7e-9fd4-94fbdd9f693a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangSmith is a framework built on LangChain, designed to track the inner workings of large language models (LLMs) and AI agents within products. It provides tools for debugging, testing, and improving the reliability of LLM applications. LangSmith allows users to analyze LLM behavior, create and run tests with existing or new datasets, and offers fine-grained controls and customizability through its SDK.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 1032, 'total_tokens': 1113}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f33667828e', 'finish_reason': 'stop', 'logprobs': None} id='run-217c84bf-4508-4500-b5ad-db8dbf1a8806-0' usage_metadata={'input_tokens': 1032, 'output_tokens': 81, 'total_tokens': 1113}\n",
            "LangSmith is a framework built on LangChain, designed to track the inner workings of large language models (LLMs) and AI agents within products. It provides tools for debugging, testing, and improving the reliability of LLM applications. LangSmith allows users to analyze LLM behavior, create and run tests with existing or new datasets, and offers fine-grained controls and customizability through its SDK.\n"
          ]
        }
      ],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']\n",
        "print(response)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "<div style=\"border: 2px solid white; background: black; padding: 10px;\">\n",
        "\n",
        "#### üèóÔ∏è Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means.\n",
        "\n",
        "#### ! Answer #1:\n",
        "\n",
        "![Sample Image](images/langsmith_3.jpg)\n",
        "\n",
        "The LangSmith interface is a bit overwhelming and I spent a while clicking around it.\n",
        "\n",
        "Looking at the Personal -> Projects lists all of the traces and evaluations that you have set up.\n",
        "\n",
        "For this project I used the name \"LangSmith - 5d73bab4\"  \n",
        "\n",
        "In the summary it tells you:\n",
        "- run count - number of times this trace has been run\n",
        "- error rate\n",
        "- total tokens used\n",
        "- total cost \n",
        "\n",
        "Drilling into the selected project you see the following screen with runs, threads, monitor and setup as main tabs.\n",
        "\n",
        "![Sample Image](images/langsmith_4.jpg)\n",
        "\n",
        "The Runs tab shows each run with the input and output. At this level, the run represents the entire chain. The top level run can be expanded to show the different steps in the chain. Each can be expanded further.\n",
        "\n",
        "Summary information is displayed on the table to the right. This run summary includes start and end time, status of the run, total tokens used, latency etc\n",
        "\n",
        "Selecting one of the runs (in this case ChatOpenAI) displays a more detail screen giving information on the step of the chain as shown below. \n",
        "\n",
        "![Sample Image](images/langsmith_5.jpg)\n",
        "\n",
        "For each step the input and output are displayed. There is also a summary with the start and end, the status of the step, number of tokens and cost of the step. It also identifies the type of run.\n",
        "\n",
        "In this case, the model represented by the ChatOpenAI was passed an input of Human information composed of the instructions, the context (as selected by the retriever) and the Question. The response has the answer provided by AI.\n",
        "\n",
        "Looking at other runns - each details their input and their output with the information depending on the type of run\n",
        "\n",
        "Each run is identified by type with an icon and a Name - Prompt, Sequence, Chain, Retriever, \n",
        "\n",
        "The retriever shows the documents that were returned by the retriever\n",
        "The prompt shows the inputs including the question and the context as well as the output - which is the final prompt sent to the model\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Loading Our Testing Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the data for the LangSmith testing from the AIMakerspace github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJ8uCbT7S1Z",
        "outputId": "cdcbe030-aae2-46da-8cb7-30811da9f87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DataRepository'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 84 (delta 23), reused 28 (delta 8), pack-reused 8 (from 1)\u001b[K\n",
            "Receiving objects: 100% (84/84), 70.08 MiB | 3.60 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI-Maker-Space/DataRepository.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w5fmy5Gy7X03"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"DataRepository/langchain_blog_test_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-aie4-triples-v7\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in test_df.iterrows():\n",
        "  triplet = triplet[1]\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"], \"context\": triplet[\"context\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "## Task 6: Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_context_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.inputs[\"context\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "501c5e204387460e8e9a585c9b2ca834",
            "323a077888c84bdfbac393d1ebef9d6c",
            "710fe93f3241401bafa10a1e0a1bda02",
            "c65a6921701742da9b67c591b19b6336",
            "17d4193e78a14050a11c5f8306b3ba0b",
            "99751d16888640078ae4820cacab5760",
            "a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "54e7ccad2e774261a3313316a1397f66",
            "41e287b8dc674f839b6485c9606207d5",
            "f7457f38948d472da4027ba1566b47e4",
            "af61b863014d4a8c960570de31a02b3f"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "872581e4-9c21-414f-d25e-9454c47c0587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-b64674cb' at:\n",
            "https://smith.langchain.com/o/c97b0028-7cab-5f76-a748-1369ba450931/datasets/7784a166-5cb4-4bf0-9152-3bdecf032df6/compare?selectedSessions=03b29b92-16b1-4b6a-92ad-8e1c7c09be73\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e933a3f3eac48689fb6f47436e8b937",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\",\n",
        "                                            config={\"llm\":base_llm},\n",
        "                                            prepare_data=prepare_context_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"llm\":base_llm,\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"llm\":base_llm,\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "unlabeled_spelling_errors_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"llm\":base_llm,\n",
        "        \"criteria\" : {\n",
        "            \"typos\" : \"Are there any typographical errors ie words spelled incorrectly?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    retrieval_augmented_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_spelling_errors_evaluator,\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "<div style=\"border: 2px solid white; background: black; padding: 10px;\">\n",
        "\n",
        "#### ‚ùìQuestion #1:\n",
        "\n",
        "![Langchain Posts](images/langsmith_2.jpg)\n",
        "\n",
        "<b>What conclusions can you draw about the above results?</b>\n",
        "\n",
        "Over 7 runs there is some variance between each run: \n",
        "\n",
        "Chain of thought contextual accuracy runs 0.66 to 0.74\n",
        "Dopeness runs 0.27 to 0.53\n",
        "Accuracy runs 5.6 to 6.5\n",
        "There were no typos\n",
        "\n",
        "This is without any changes to the data, the processing, or the evaluation criteria. So this is a bit surprising. The graph above shows this variation.\n",
        "\n",
        "Dopeness swings from 6 Y to 12 Y so there is a lot of variability here. Contextual accuracy varies drom 15 to 17 correct so not so much variabe - which I guessis more reasonable since Dopeness is a much more subjective evaluation. \n",
        "\n",
        "<b>Describe in your own words what the metrics are expressing.</b>\n",
        "\n",
        "COT Contextual Accuracy - checks how accurate the answer is based on the chain of thought. Either correct or incorrect.\n",
        "\n",
        "Accuracy - how accurate the answer is and whether it is missong any specific details. Rating of 1-10\n",
        "\n",
        "Dopeness - answers are checked for accuracy, clarity, engagement (language of the answer, how \"exciting\"), completeness (does it fully answer the question), overall impact (does it evoke emotional reaction). Provides a Y or N score. However it seems that different questions have slightly different evaluation criteria for dopeness which seems concerning.\n",
        " \n",
        "Accuracy - this provides a numerical score as to how accurate the answer matches the reference answer. It also checks how different the answer is and how the answer is phrased to determine how accurate it is. Low scores are given if the response does not answer the question. It is also looking for context and coherent explanations. Seems to also look for clarity. Scores appear to be on a scale 1 to 10.\n",
        "\n",
        "I experimented with grade level and never got outside of a Y or N answer. I also put in an evaluation for Typos which it found none (but labeled as Y). It is interesting reading the explanations how the text was checked for accuracy.\n",
        "\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17d4193e78a14050a11c5f8306b3ba0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323a077888c84bdfbac393d1ebef9d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99751d16888640078ae4820cacab5760",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "value": ""
          }
        },
        "41e287b8dc674f839b6485c9606207d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "501c5e204387460e8e9a585c9b2ca834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_323a077888c84bdfbac393d1ebef9d6c",
              "IPY_MODEL_710fe93f3241401bafa10a1e0a1bda02",
              "IPY_MODEL_c65a6921701742da9b67c591b19b6336"
            ],
            "layout": "IPY_MODEL_17d4193e78a14050a11c5f8306b3ba0b"
          }
        },
        "54e7ccad2e774261a3313316a1397f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "710fe93f3241401bafa10a1e0a1bda02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e7ccad2e774261a3313316a1397f66",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e287b8dc674f839b6485c9606207d5",
            "value": 1
          }
        },
        "99751d16888640078ae4820cacab5760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f5f4beaa8f4a3b94002c5ed122c2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af61b863014d4a8c960570de31a02b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65a6921701742da9b67c591b19b6336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7457f38948d472da4027ba1566b47e4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_af61b863014d4a8c960570de31a02b3f",
            "value": "‚Äá23/?‚Äá[02:10&lt;00:00,‚Äá‚Äá5.36s/it]"
          }
        },
        "f7457f38948d472da4027ba1566b47e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
