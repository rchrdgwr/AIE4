{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -qU langchain langchain-openai langchain-cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgFAXWVW3wm",
        "outputId": "636db35c-f05a-4038-ec7a-02360bef2dae"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqYM4Eoxcov"
      },
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key and LangSmith key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "0ce6514e-2479-4001-af24-824f987ce599"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2024, 9, 26, 21, 26, 25, 456154)}, page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content: : 5\n",
            "Review: Ultra-violent first entry with lots of killings, thrills , noisy action , suspense , and crossfire . In this original John Wick (2014) , an ex-hit-man comes out of retirement to track down the gangsters that killed his dog and took everything from him . With the untimely death of his beloved wife still bitter in his mouth he seeks for vengeance . But when an arrogant Russian mob prince and hoodlums steal his car and kill his dog , they are fully aware of his lethal capacity. The Bogeyman will find himself dragged into an impossible task as every killer in the business dreams of cornering the legendary Wick who now has an enormous price on his head . In this first installment John Wick , blind with revenge, and for his salvation John will immediately unleash a carefully orchestrated maelstrom of destruction against those attempt to chase him and with a price tag on his head, as he is the target of hit men : an army of bounty-hunting killers on his trail and a murderer woman everywhere . The legendary hitman will be forced to unearth his meticulously concealed identity and to carry out a relentless vendetta . Now, only blood can quench the boogeyman's thirst for retribution . Don't Set Him Off! . John Wick isn't the Boogeyman... He's the guy you send to kill the doomed Boogeyman. Revenge is all he has left. You want peace, prepare for war . Don't Hunt What You Can't Kill. Tick Tock, Mr. Wick. Everyone Is Waiting. For John Wick . Every Action Has Consequences. This Friday, Wick is Back . Its the World Vs. Wick. Every Action Has Consequences.\n",
            "metadata: {'source': 'john_wick_1.csv', 'row': 5, 'Review_Date': '23 March 2023', 'Review_Title': ' Violent and gripping story with plenty of unstopped action , shootouts and breathtaking fights\\n', 'Review_Url': '/review/rw8945545/?ref_=tt_urv', 'Author': 'ma-cortes', 'Rating': 7, 'Movie_Title': 'John Wick 1', 'last_accessed_at': '2024-09-27T19:47:21.679993', '_id': '600923f52e0f4c338db7ea8fa34aaa6e', '_collection_name': 'JohnWick'}\n",
            "---\n",
            "* : 2\n",
            "Review: The first three John Wick films came in fairly quick succession, with only five years between the first and third chapters, each film trying to top the one before in terms of wild set pieces. For me, it all became a bit too much, the action rapidly becoming too frenetic and over-the-top. By Parabellum (part three), I had had enough. [{'source': 'john_wick_4.csv', 'row': 2, 'Review_Date': '25 March 2023', 'Review_Title': ' It got on my wick.\\n', 'Review_Url': '/review/rw8950606/?ref_=tt_urv', 'Author': 'BA_Harrison', 'Rating': 4, 'Movie_Title': 'John Wick 4', 'last_accessed_at': '2024-09-30T19:47:21.682097', '_id': '05f88fba54ed4fbe890343ce8ae8ac89', '_collection_name': 'JohnWick'}]\n",
            "* : 19\n",
            "Review: The inevitable third chapter of the JOHN WICK franchise continues on a high with the same quality as the last. In fact, this may just pip that one to the post to be the best JOHN WICK yet. The pacing is spot on and the plotting is measured by a huge amount of action sequences, all of which fizz and crackle with skill and energy. The movie hits the ground running as Keanu faces off with a hulking character in a library and then Triads in a weapon shop, but what really amazes here is the sheer inventiveness of the non-repetitive action. There are horse and motorbike chases, a Moroccan brawl with fighting dogs, alongside the massive action of the climax. There also seems to be a greater emphasis on hand-to-hand combat, featuring the likes of RAID actors and old-timer Mark Dacascos, which is a real pleasure as the choreography is top-notch. In terms of sheer enjoyment, this is a film hard to beat, and to criticise it at all is unnecessary. Bring on the fourth! [{'source': 'john_wick_3.csv', 'row': 19, 'Review_Date': '22 September 2019', 'Review_Title': ' Sheer enjoyment\\n', 'Review_Url': '/review/rw5135863/?ref_=tt_urv', 'Author': 'Leofwine_draca', 'Rating': 9, 'Movie_Title': 'John Wick 3', 'last_accessed_at': '2024-09-29T19:47:21.681469', '_id': 'c186861a9dca4fa69bd0ead766b36b86', '_collection_name': 'JohnWick'}]\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "top_k = 1\n",
        "retrieved_documents = retriever.invoke(\"WHow many people killed in John Wick?\")\n",
        "for doc in retrieved_documents:\n",
        "    print(f\"content: {doc.page_content}\")\n",
        "    print(f\"metadata: {doc.metadata}\")\n",
        "    print(\"---\")\n",
        "\n",
        "results = vectorstore.similarity_search(\n",
        "    \"What weapons were used in the third movie\", k=2\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\''"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hit-man comes out of retirement to seek revenge on the gangsters who killed his dog and took everything from him. This leads to a series of violent and action-packed confrontations as he faces off against various enemies.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Some people liked John Wick, while others did not. It seems to have mixed reviews.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 4\" by the author jtindahouse. The URL to that review is: \\'/review/rw8946038/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, the action is beautifully choreographed, the setup is surprisingly emotional for an action flick, and Keanu Reeves delivers a great performance. If you love action movies, you will enjoy John Wick.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note\n",
        "\n",
        "I could not get this to work within the confines of the versions working with the other methods\n",
        "\n",
        "I did work with this in another notebook and failed to find the correct combination of versions\n",
        "\n",
        "I elected to ignore this retrieval method to ensure completion of the assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "no validator found for <class 'pydantic.types.SecretStr'>, see `arbitrary_types_allowed` in Config",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[91], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontextual_compression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextualCompressionRetriever\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CohereRerank\n\u001b[1;32m      4\u001b[0m compressor \u001b[38;5;241m=\u001b[39m CohereRerank(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrerank-english-v3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m compression_retriever \u001b[38;5;241m=\u001b[39m ContextualCompressionRetriever(\n\u001b[1;32m      6\u001b[0m     base_compressor\u001b[38;5;241m=\u001b[39mcompressor, base_retriever\u001b[38;5;241m=\u001b[39mnaive_retriever\n\u001b[1;32m      7\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/langchain_cohere/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarize_chain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_summarize_chain\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatCohere\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcohere_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_cohere_tools_agent\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/langchain_cohere/chains/summarize/summarize_chain.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableLambda, RunnableSerializable\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RAG_SUMMARIZATION_PREAMBLE\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatCohere\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_summarize_prompt\u001b[39m(\n\u001b[1;32m     22\u001b[0m     prompt_message: BaseMessage \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[1;32m     23\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease summarize the documents in a concise manner.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     ),\n\u001b[1;32m     25\u001b[0m     extra_prompt_messages: List[BaseMessagePromptTemplate] \u001b[38;5;241m=\u001b[39m [],\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create prompt for this agent.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m        system_message: Message to use as the system message that will be the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m        A prompt template to pass into this agent.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/langchain_cohere/chat_models.py:58\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, ConfigDict, PrivateAttr\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcohere_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     55\u001b[0m     _convert_to_cohere_tool,\n\u001b[1;32m     56\u001b[0m     _format_to_cohere_tools,\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCohere\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_cohere\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreact_multi_hop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_documents\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_to_cohere_tool_results\u001b[39m(\n\u001b[1;32m     63\u001b[0m     messages: List[BaseMessage], tool_message_index: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     64\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/langchain_cohere/llms.py:56\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseCohere\u001b[39;00m(Serializable):\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Base class for Cohere models.\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     client: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m#: :meta private:\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/pydantic/v1/main.py:197\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         is_untouched(value)\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ann_type \u001b[38;5;241m!=\u001b[39m PyObject\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     ):\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     fields[ann_name] \u001b[38;5;241m=\u001b[39m ModelField\u001b[38;5;241m.\u001b[39minfer(\n\u001b[1;32m    198\u001b[0m         name\u001b[38;5;241m=\u001b[39mann_name,\n\u001b[1;32m    199\u001b[0m         value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    200\u001b[0m         annotation\u001b[38;5;241m=\u001b[39mann_type,\n\u001b[1;32m    201\u001b[0m         class_validators\u001b[38;5;241m=\u001b[39mvg\u001b[38;5;241m.\u001b[39mget_validators(ann_name),\n\u001b[1;32m    202\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ann_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39munderscore_attrs_are_private:\n\u001b[1;32m    205\u001b[0m     private_attributes[ann_name] \u001b[38;5;241m=\u001b[39m PrivateAttr()\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/pydantic/v1/fields.py:504\u001b[0m, in \u001b[0;36mModelField.infer\u001b[0;34m(cls, name, value, annotation, class_validators, config)\u001b[0m\n\u001b[1;32m    501\u001b[0m     required \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    502\u001b[0m annotation \u001b[38;5;241m=\u001b[39m get_annotation_from_field_info(annotation, field_info, name, config\u001b[38;5;241m.\u001b[39mvalidate_assignment)\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    505\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    506\u001b[0m     type_\u001b[38;5;241m=\u001b[39mannotation,\n\u001b[1;32m    507\u001b[0m     alias\u001b[38;5;241m=\u001b[39mfield_info\u001b[38;5;241m.\u001b[39malias,\n\u001b[1;32m    508\u001b[0m     class_validators\u001b[38;5;241m=\u001b[39mclass_validators,\n\u001b[1;32m    509\u001b[0m     default\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    510\u001b[0m     default_factory\u001b[38;5;241m=\u001b[39mfield_info\u001b[38;5;241m.\u001b[39mdefault_factory,\n\u001b[1;32m    511\u001b[0m     required\u001b[38;5;241m=\u001b[39mrequired,\n\u001b[1;32m    512\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    513\u001b[0m     field_info\u001b[38;5;241m=\u001b[39mfield_info,\n\u001b[1;32m    514\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/pydantic/v1/fields.py:434\u001b[0m, in \u001b[0;36mModelField.__init__\u001b[0;34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m SHAPE_SINGLETON\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mprepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare()\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/pydantic/v1/fields.py:555\u001b[0m, in \u001b[0;36mModelField.prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;129;01mis\u001b[39;00m Undefined \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulate_validators()\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/pydantic/v1/fields.py:829\u001b[0m, in \u001b[0;36mModelField.populate_validators\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_fields \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m SHAPE_GENERIC:\n\u001b[1;32m    826\u001b[0m     get_validators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__get_validators__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    827\u001b[0m     v_funcs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;241m*\u001b[39m[v\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m class_validators_ \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39meach_item \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39mpre],\n\u001b[0;32m--> 829\u001b[0m         \u001b[38;5;241m*\u001b[39m(get_validators() \u001b[38;5;28;01mif\u001b[39;00m get_validators \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(find_validators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config))),\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;241m*\u001b[39m[v\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m class_validators_ \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39meach_item \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mpre],\n\u001b[1;32m    831\u001b[0m     )\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidators \u001b[38;5;241m=\u001b[39m prep_validators(v_funcs)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_validators \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m~/anaconda3/envs/jupyter_2/lib/python3.11/site-packages/pydantic/v1/validators.py:765\u001b[0m, in \u001b[0;36mfind_validators\u001b[0;34m(type_, config)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m make_arbitrary_type_validator(type_)\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno validator found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, see `arbitrary_types_allowed` in Config\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: no validator found for <class 'pydantic.types.SecretStr'>, see `arbitrary_types_allowed` in Config"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: langchain\n",
            "Version: 0.2.16\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /home/rchrdgwr/anaconda3/envs/jupyter_2/lib/python3.11/site-packages\n",
            "Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-community, ragas\n",
            "---\n",
            "Name: langchain-cohere\n",
            "Version: 0.3.0\n",
            "Summary: An integration package connecting Cohere and LangChain\n",
            "Home-page: https://github.com/langchain-ai/langchain-cohere\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /home/rchrdgwr/anaconda3/envs/jupyter_2/lib/python3.11/site-packages\n",
            "Requires: cohere, langchain-core, langchain-experimental, pandas, pydantic, tabulate\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show langchain langchain-cohere\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick. It was described as the coolest action film of the year, slick, violent fun, and a must-see for action fans.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: /review/rw4854296/?ref_=tt_urv'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, the main character, played by Keanu Reeves, is forced back into the world of crime and assassination after the mobster Santino D'Antonio seeks his help and blows up his house when he refuses. Wick is given a contract to kill Santino's sister in Rome, leading to a series of events where he becomes the target of professional killers. Ultimately, Wick seeks revenge on Santino for betraying him.\""
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, there is a review with a rating of 10. Here is the URL to that review: '/review/rw4854296/?ref_=tt_urv'\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" the main character, John Wick, seeks revenge after his wife dies and his dog is killed by Russian mobsters who also steal his car. Wick, revealed to be a super-assassin, goes on a mission to take down the mobster\\'s gang and seek justice for his losses. The film is known for its intense action sequences and stylish stunts.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_160982/3574430551.py:8: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-qdrant package and should be used instead. To use it run `pip install -U langchain-qdrant` and import as `from langchain_qdrant import Qdrant`.\n",
            "  parent_document_vectorstore = Qdrant(\n"
          ]
        }
      ],
      "source": [
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People's opinions on John Wick seem to vary. Some individuals like the series and find it consistent and well-received, while others have negative opinions about it. It ultimately depends on personal preferences.\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review: /review/rw4854296/?ref_=tt_urv'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, the main character, played by Keanu Reeves, is a retired assassin who comes out of retirement after someone kills his dog and steals his car. He is then called on to pay off an old debt by helping Ian McShane take over the Assassin's Guild by traveling to Italy, Canada, and Manhattan and killing numerous assassins.\""
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever,  multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the reviews provided, it seems that generally, people liked John Wick. The majority of reviews praise its action sequences, Keanu Reeves' performance, stylishness, and fun factor. Overall, it appears that John Wick was well-received by fans of action films.\""
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3.\" Here is the URL to that review:\\n/review/rw4854296/?ref_=tt_urv'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, an ex-hitman comes out of retirement to seek vengeance against the gangsters who killed his dog and took everything from him. The movie is filled with violent action, shootouts, and breathtaking fights as John Wick unleashes a maelstrom of destruction against those who try to chase him. The plot revolves around John Wick's quest for revenge and the consequences of his actions.\""
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dHeB-yGXneL",
        "outputId": "efc59105-518a-4134-9228-d98b8a97e08e"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWickSemantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" an ex-hitman comes out of retirement to seek vengeance against the gangsters who killed his dog and took everything from him. The story follows John Wick as he unleashes a maelstrom of destruction against those who come after him, leading to a relentless vendetta. The movie is filled with action, suspense, shootouts, and breathtaking fights as John Wick navigates through a world where every action has consequences.'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LangSmith for Tracing\n",
        "\n",
        "We will set up LangSmith to trace our LLM calls to allow us to analyze latency and costs\n",
        "\n",
        "Set the project name\n",
        "\n",
        "Originally I could not get tracing to appear in LangSmith I moved on to at least accomplish something\n",
        "\n",
        "However later after deleting all LangSmith keys and creating a new one I did get this to work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your langsmith key is: AIE4 - A14 - BM25 c1520bab\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - A14 - BM25 {uuid4().hex[0:8]}\"\n",
        "\n",
        "print(f\"Your langsmith key is: {os.environ['LANGCHAIN_PROJECT']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"In John Wick, the character who dies is John Wick's dog.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 3905, 'total_tokens': 3919, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9110893c-b297-47dc-ae96-7b7d80bd190c-0' usage_metadata={'input_tokens': 3905, 'output_tokens': 14, 'total_tokens': 3919}\n",
            "In John Wick, the character who dies is John Wick's dog.\n"
          ]
        }
      ],
      "source": [
        "response = naive_retrieval_chain.invoke({\"question\" : \"Who dies in John Wick?\"})['response']\n",
        "print(response)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets look at the documents - kind of curious how big they are, how variable in length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "599\n",
            "Average Content Length: 533.9\n",
            "Minimum Content Length: 29\n",
            "Maximum Content Length: 2440\n"
          ]
        }
      ],
      "source": [
        "documents[0]\n",
        "print(len(documents))\n",
        "content_1 = documents[0].page_content\n",
        "print(len(content_1))\n",
        "lengths = [len(doc.page_content) for doc in documents]\n",
        "average_length = sum(lengths) / len(lengths) if lengths else 0\n",
        "min_length = min(lengths) if lengths else 0\n",
        "max_length = max(lengths) if lengths else 0\n",
        "\n",
        "print(f\"Average Content Length: {average_length}\")\n",
        "print(f\"Minimum Content Length: {min_length}\")\n",
        "print(f\"Maximum Content Length: {max_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Question Generation\n",
        "\n",
        "We will use RAGAS to create the questions from the context\n",
        "\n",
        "Set up the RAGAS generator\n",
        "\n",
        "Use gpt-3.5-turbo for the generator - reasonable powerful LLM\n",
        "\n",
        "Use gpt-4o-mini as the critic - strong, has some reasoning, cheap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm,\n",
        "    critic_llm,\n",
        "    embeddings\n",
        ")\n",
        "\n",
        "distributions = {\n",
        "    simple: 0.5,\n",
        "    multi_context: 0.4,\n",
        "    reasoning: 0.1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions to create the questions using RAGAS, save them to file and restore them from file.\n",
        "\n",
        "Saved offline to be used for subsequent runs since we want consistent questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "import pickle\n",
        "\n",
        "chunk_size = 1000\n",
        "chunk_overlap = 100\n",
        "file_path = 'ragas_testset.pkl'\n",
        "\n",
        "# load an existing ragas testset\n",
        "def load_ragas_testset_if_exists():\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                ragas_state = pickle.load(f)\n",
        "            print(f\"Ragas testset loaded from {file_path}\")\n",
        "            return ragas_state\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading ragas testset: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"No existing ragas tesetset found at {file_path}\")\n",
        "        return None\n",
        "\n",
        "# Save the ragas testset\n",
        "def save_ragas_testset(testset):\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(testset, f)\n",
        "        print(f\"Ragas testset saved to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving ragas testset: {e}\")\n",
        "\n",
        "\n",
        "# create questions\n",
        "def create_questions_for_ragas(documents, num_questions=1):\n",
        "    generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "    critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    generator = TestsetGenerator.from_langchain(\n",
        "        generator_llm,\n",
        "        critic_llm,\n",
        "        embeddings\n",
        "    )\n",
        "    distributions = {\n",
        "        simple: 0.5,\n",
        "        multi_context: 0.4,\n",
        "        reasoning: 0.1\n",
        "    }\n",
        "    \n",
        "    testset = generator.generate_with_langchain_docs(documents, num_questions, distributions, with_debugging_logs=False)\n",
        "    save_ragas_testset(testset)\n",
        "    return testset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Routine to get the questions or create them\n",
        "\n",
        "Keep create_questions = False unless we need to recreate the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ragas testset loaded from ragas_testset.pkl\n"
          ]
        }
      ],
      "source": [
        "create_questions = False\n",
        "ragas_testset = None\n",
        "num_questions = 40\n",
        "if create_questions:\n",
        "    ragas_testset = create_questions_for_ragas(documents, num_questions)\n",
        "else:\n",
        "    ragas_testset = load_ragas_testset_if_exists()\n",
        "if ragas_testset:\n",
        "    ragas_testset.to_pandas()\n",
        "else:\n",
        "    print(\"No RAGAS testset found - need to create questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at some of the questions pulled in from offline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>evolution_type</th>\n",
              "      <th>metadata</th>\n",
              "      <th>episode_done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What makes John Wick stand out as a favorite r...</td>\n",
              "      <td>[: 22\\nReview: John Wick is one of my favourit...</td>\n",
              "      <td>John Wick stands out as a favorite recent year...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'john_wick_2.csv', 'row': 22, 'Rev...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are some examples of classic internationa...</td>\n",
              "      <td>[: 11\\nReview: JOHN WICK is a rare example of ...</td>\n",
              "      <td>Some examples of classic international films s...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'john_wick_1.csv', 'row': 11, 'Rev...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What was the surprise hit movie starring Keanu...</td>\n",
              "      <td>[: 6\\nReview: In 2014, a Keanu Reeves revenge ...</td>\n",
              "      <td>John Wick</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'john_wick_2.csv', 'row': 6, 'Revi...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who does John Wick face as he is called upon b...</td>\n",
              "      <td>[: 5\\nReview: Iosef's uncle still has John Wic...</td>\n",
              "      <td>John Wick faces deadly assassins, numerous kil...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'john_wick_2.csv', 'row': 5, 'Revi...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the genre and main actor of the film \"...</td>\n",
              "      <td>[: 21\\nReview: John Wick is an action film wit...</td>\n",
              "      <td>The genre of the film 'John Wick' is action, a...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'john_wick_1.csv', 'row': 21, 'Rev...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What makes John Wick stand out as a favorite r...   \n",
              "1  What are some examples of classic internationa...   \n",
              "2  What was the surprise hit movie starring Keanu...   \n",
              "3  Who does John Wick face as he is called upon b...   \n",
              "4  What is the genre and main actor of the film \"...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [: 22\\nReview: John Wick is one of my favourit...   \n",
              "1  [: 11\\nReview: JOHN WICK is a rare example of ...   \n",
              "2  [: 6\\nReview: In 2014, a Keanu Reeves revenge ...   \n",
              "3  [: 5\\nReview: Iosef's uncle still has John Wic...   \n",
              "4  [: 21\\nReview: John Wick is an action film wit...   \n",
              "\n",
              "                                        ground_truth evolution_type  \\\n",
              "0  John Wick stands out as a favorite recent year...         simple   \n",
              "1  Some examples of classic international films s...         simple   \n",
              "2                                          John Wick         simple   \n",
              "3  John Wick faces deadly assassins, numerous kil...         simple   \n",
              "4  The genre of the film 'John Wick' is action, a...         simple   \n",
              "\n",
              "                                            metadata  episode_done  \n",
              "0  [{'source': 'john_wick_2.csv', 'row': 22, 'Rev...          True  \n",
              "1  [{'source': 'john_wick_1.csv', 'row': 11, 'Rev...          True  \n",
              "2  [{'source': 'john_wick_2.csv', 'row': 6, 'Revi...          True  \n",
              "3  [{'source': 'john_wick_2.csv', 'row': 5, 'Revi...          True  \n",
              "4  [{'source': 'john_wick_1.csv', 'row': 21, 'Rev...          True  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ragas_testset.to_pandas().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer Generation\n",
        "\n",
        "Routine to generate answers by invoking the passed in chain\n",
        "\n",
        "Use the previously generated questions to determine answers. We then have the questions, the answer, the context and the ground truth.\n",
        "\n",
        "Also lets calculate latency (time for the invoke to run) and number of tokens used. This will allow us to compare the different chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datasets import Dataset\n",
        "from langsmith import traceable\n",
        "\n",
        "@traceable(\n",
        "        run_type=\"llm\",\n",
        "        name=\"generate_answers\",\n",
        "        project_name=\"Random\"\n",
        ")\n",
        "def generate_answers(name, chain, testset):\n",
        "    answers = []\n",
        "    contexts = []\n",
        "    questions = testset.to_pandas()[\"question\"].values.tolist()\n",
        "    ground_truths = testset.to_pandas()[\"ground_truth\"].values.tolist()\n",
        "    latencies = []\n",
        "    tokens = []\n",
        "\n",
        "    for question in questions:\n",
        "        start_time = time.time()  \n",
        "        answer = chain.invoke({\"question\" : question})\n",
        "        latency = time.time() - start_time\n",
        "        answers.append(answer[\"response\"].content)\n",
        "        contexts.append([context.page_content for context in answer[\"context\"]])\n",
        "        latencies.append(latency)\n",
        "        total_tokens = answer[\"response\"].response_metadata[\"token_usage\"][\"total_tokens\"]\n",
        "        tokens.append(total_tokens)\n",
        "    print(\"Answers generated\")\n",
        "    return Dataset.from_dict({\n",
        "        \"question\" : questions,\n",
        "        \"answer\" : answers,\n",
        "        \"contexts\" : contexts,\n",
        "        \"ground_truth\" : ground_truths,\n",
        "        \"latency\": latencies,\n",
        "        \"tokens\": tokens,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate answers for the naive retrieval chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers generated\n"
          ]
        }
      ],
      "source": [
        "naive_retrieval_chain_dataset = generate_answers(\"Naive_retrieval\", naive_retrieval_chain, ragas_testset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets look at some of the responses we get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>latency</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What makes John Wick stand out as a favorite r...</td>\n",
              "      <td>John Wick stands out as a favorite recent year...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>John Wick stands out as a favorite recent year...</td>\n",
              "      <td>3.196085</td>\n",
              "      <td>3571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are some examples of classic internationa...</td>\n",
              "      <td>Some classic international films similar to JO...</td>\n",
              "      <td>[: 11\\nReview: JOHN WICK is a rare example of ...</td>\n",
              "      <td>Some examples of classic international films s...</td>\n",
              "      <td>0.953689</td>\n",
              "      <td>3875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What was the surprise hit movie starring Keanu...</td>\n",
              "      <td>The surprise hit movie starring Keanu Reeves i...</td>\n",
              "      <td>[: 6\\nReview: In 2014, a Keanu Reeves revenge ...</td>\n",
              "      <td>John Wick</td>\n",
              "      <td>0.769292</td>\n",
              "      <td>3015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who does John Wick face as he is called upon b...</td>\n",
              "      <td>John Wick faces deadly assassins, numerous kil...</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>John Wick faces deadly assassins, numerous kil...</td>\n",
              "      <td>0.967237</td>\n",
              "      <td>3763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the genre and main actor of the film \"...</td>\n",
              "      <td>The genre of the film \"John Wick\" is action, a...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>The genre of the film 'John Wick' is action, a...</td>\n",
              "      <td>0.790676</td>\n",
              "      <td>3904</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What makes John Wick stand out as a favorite r...   \n",
              "1  What are some examples of classic internationa...   \n",
              "2  What was the surprise hit movie starring Keanu...   \n",
              "3  Who does John Wick face as he is called upon b...   \n",
              "4  What is the genre and main actor of the film \"...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  John Wick stands out as a favorite recent year...   \n",
              "1  Some classic international films similar to JO...   \n",
              "2  The surprise hit movie starring Keanu Reeves i...   \n",
              "3  John Wick faces deadly assassins, numerous kil...   \n",
              "4  The genre of the film \"John Wick\" is action, a...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "1  [: 11\\nReview: JOHN WICK is a rare example of ...   \n",
              "2  [: 6\\nReview: In 2014, a Keanu Reeves revenge ...   \n",
              "3  [: 20\\nReview: After resolving his issues with...   \n",
              "4  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "\n",
              "                                        ground_truth   latency  tokens  \n",
              "0  John Wick stands out as a favorite recent year...  3.196085    3571  \n",
              "1  Some examples of classic international films s...  0.953689    3875  \n",
              "2                                          John Wick  0.769292    3015  \n",
              "3  John Wick faces deadly assassins, numerous kil...  0.967237    3763  \n",
              "4  The genre of the film 'John Wick' is action, a...  0.790676    3904  "
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain_dataset.to_pandas().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation\n",
        "\n",
        "Use RAGAS to determine context precision and context recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.metrics import (\n",
        "\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "from ragas import evaluate\n",
        "def evaluate_ragas_results(dataset):\n",
        "    results = evaluate(\n",
        "        dataset,\n",
        "        metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        ],\n",
        "    )\n",
        "    print(\"Evaluation complete\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the evaluation for the naive retrieval chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87c58d2daa16479fbd637c0faa4e2a1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation complete\n"
          ]
        }
      ],
      "source": [
        "naive_retrieval_chain_evaluation = evaluate_ragas_results(naive_retrieval_chain_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Merge and Offline Storage\n",
        "\n",
        "Merge the answer dataset and the evaluation dataset to combine the latency, the tokens, the context precision and the context recall\n",
        "\n",
        "Save this combined dataset offline for later recall in case we have program failures due to installing software that causes conflicts - yes this has happened way too many times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "def save_datasets(name, dataset, evaluation):\n",
        "    os.makedirs(\"datasets\", exist_ok=True)\n",
        "    d1 = dataset.to_pandas()\n",
        "    d2 = evaluation.to_pandas()\n",
        "    d2_d1 = pd.merge(d2, d1[[\"question\",\"latency\", \"tokens\"]], on=\"question\", how=\"outer\")\n",
        "    file_name = f\"datasets/{name}.csv\"\n",
        "    d2_d1.to_csv(file_name, index=False, encoding='utf-8')\n",
        "    print(\"Datasets merged and saved offline\")\n",
        "    return d2_d1\n",
        "def read_dataset(name):\n",
        "    file_name = f\"datasets/{name}.csv\"\n",
        "    dataset = pd.read_csv(file_name)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge and save the naive retrieval dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets merged and saved offline\n"
          ]
        }
      ],
      "source": [
        "naive_retrieval_dataset = save_datasets(\"Naive_retrieval\",naive_retrieval_chain_dataset,naive_retrieval_chain_evaluation )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the naive retrieval dataset to ensure the process is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "naive_retrieval_dataset = read_dataset(\"Naive_retrieval\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Calculate and Display Metrics\n",
        "\n",
        "Display the averges for:\n",
        "- context precision\n",
        "- context recall\n",
        "- latency\n",
        "- tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_averages(name, df):\n",
        "    averages = {\n",
        "        \"Average Context Precision\": df[\"context_precision\"].mean(),\n",
        "        \"Average Context Recall\": df[\"context_recall\"].mean(),\n",
        "        \"Average Latency (ms)\": df[\"latency\"].mean(),\n",
        "        \"Average Tokens\": df[\"tokens\"].mean()\n",
        "    }\n",
        "    averages_df = pd.DataFrame(list(averages.items()), columns=[\"Metric\", name])\n",
        "    print(averages_df)\n",
        "    return averages_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the metrics for the naive retrieval chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      Metric  Naive_retrieval\n",
            "0  Average Context Precision         0.733476\n",
            "1     Average Context Recall         0.936937\n",
            "2       Average Latency (ms)         1.486473\n",
            "3             Average Tokens      3620.405405\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Naive_retrieval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Average Context Precision</td>\n",
              "      <td>0.733476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Average Context Recall</td>\n",
              "      <td>0.936937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average Latency (ms)</td>\n",
              "      <td>1.486473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Average Tokens</td>\n",
              "      <td>3620.405405</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Metric  Naive_retrieval\n",
              "0  Average Context Precision         0.733476\n",
              "1     Average Context Recall         0.936937\n",
              "2       Average Latency (ms)         1.486473\n",
              "3             Average Tokens      3620.405405"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_averages(\"Naive_retrieval\", naive_retrieval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BM25\n",
        "\n",
        "Lets check the BM25 retrieval chain through all of the routines and display the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers generated\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e771dab106d5433ea4443e0aefd68c4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation complete\n",
            "Datasets merged and saved offline\n",
            "                      Metric  BM25_retrieval\n",
            "0  Average Context Precision        0.656907\n",
            "1     Average Context Recall        0.783784\n",
            "2       Average Latency (ms)        1.033972\n",
            "3             Average Tokens     1275.081081\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>BM25_retrieval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Average Context Precision</td>\n",
              "      <td>0.656907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Average Context Recall</td>\n",
              "      <td>0.783784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average Latency (ms)</td>\n",
              "      <td>1.033972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Average Tokens</td>\n",
              "      <td>1275.081081</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Metric  BM25_retrieval\n",
              "0  Average Context Precision        0.656907\n",
              "1     Average Context Recall        0.783784\n",
              "2       Average Latency (ms)        1.033972\n",
              "3             Average Tokens     1275.081081"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain_dataset = generate_answers(\"BM25_retrieval\", bm25_retrieval_chain, ragas_testset)\n",
        "bm25_retrieval_chain_evaluation = evaluate_ragas_results(bm25_retrieval_chain_dataset)\n",
        "bm25_retrieval_dataset = save_datasets(\"BM25_retrieval\",bm25_retrieval_chain_dataset,bm25_retrieval_chain_evaluation )\n",
        "get_averages(\"BM25_retrieval\", bm25_retrieval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi Query Retrieval\n",
        "\n",
        "Lets check the multi query retrieval chain and display the metrics\n",
        "\n",
        "Note - we took some rate limits reached on some of the LLM calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers generated\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cd9dea451b14b7794aa80ef9faaed37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation complete\n",
            "Datasets merged and saved offline\n",
            "                      Metric  MultiQuery_retrieval\n",
            "0  Average Context Precision              0.747101\n",
            "1     Average Context Recall              0.959459\n",
            "2       Average Latency (ms)              3.323619\n",
            "3             Average Tokens           4465.189189\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>MultiQuery_retrieval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Average Context Precision</td>\n",
              "      <td>0.747101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Average Context Recall</td>\n",
              "      <td>0.959459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average Latency (ms)</td>\n",
              "      <td>3.323619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Average Tokens</td>\n",
              "      <td>4465.189189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Metric  MultiQuery_retrieval\n",
              "0  Average Context Precision              0.747101\n",
              "1     Average Context Recall              0.959459\n",
              "2       Average Latency (ms)              3.323619\n",
              "3             Average Tokens           4465.189189"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain_dataset = generate_answers(\"MultiQuery_retrieval\", multi_query_retrieval_chain, ragas_testset)\n",
        "multi_query_retrieval_chain_evaluation = evaluate_ragas_results(multi_query_retrieval_chain_dataset)\n",
        "multi_query_retrieval_dataset = save_datasets(\"MultiQuery_retrieval\",multi_query_retrieval_chain_dataset,multi_query_retrieval_chain_evaluation )\n",
        "get_averages(\"MultiQuery_retrieval\", multi_query_retrieval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parent Document Retrieval\n",
        "\n",
        "Lets check the parent document retrieval chain and display the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers generated\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "251d00996de04537864aa1cb8e8df404",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation complete\n",
            "Datasets merged and saved offline\n",
            "                      Metric  ParentDocument_retrieval\n",
            "0  Average Context Precision                  0.756006\n",
            "1     Average Context Recall                  0.765766\n",
            "2       Average Latency (ms)                  1.311893\n",
            "3             Average Tokens                648.513514\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>ParentDocument_retrieval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Average Context Precision</td>\n",
              "      <td>0.756006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Average Context Recall</td>\n",
              "      <td>0.765766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average Latency (ms)</td>\n",
              "      <td>1.311893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Average Tokens</td>\n",
              "      <td>648.513514</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Metric  ParentDocument_retrieval\n",
              "0  Average Context Precision                  0.756006\n",
              "1     Average Context Recall                  0.765766\n",
              "2       Average Latency (ms)                  1.311893\n",
              "3             Average Tokens                648.513514"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain_dataset = generate_answers(\"ParentDocument_retrieval\", parent_document_retrieval_chain, ragas_testset)\n",
        "parent_document_retrieval_chain_evaluation = evaluate_ragas_results(parent_document_retrieval_chain_dataset)\n",
        "parent_document_retrieval_dataset = save_datasets(\"ParentDocument_retrieval\",parent_document_retrieval_chain_dataset,parent_document_retrieval_chain_evaluation )\n",
        "get_averages(\"ParentDocument_retrieval\", parent_document_retrieval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ensemble Retrieval\n",
        "\n",
        "Lets check the ensemble retrieval chain and display the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers generated\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db665490cf234ba8813d55cde0b9661d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[36]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 199271, Requested 1729. Please try again in 300ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[11]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 197724, Requested 4357. Please try again in 624ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[13]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 198349, Requested 3682. Please try again in 609ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[41]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-dm9dvvnDgfJGEGv0fE2Q952w on tokens per min (TPM): Limit 200000, Used 198674, Requested 4417. Please try again in 927ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation complete\n",
            "Datasets merged and saved offline\n",
            "                      Metric  Ensemble_retrieval\n",
            "0  Average Context Precision            0.768539\n",
            "1     Average Context Recall            0.931373\n",
            "2       Average Latency (ms)            3.769606\n",
            "3             Average Tokens         5328.081081\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Ensemble_retrieval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Average Context Precision</td>\n",
              "      <td>0.768539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Average Context Recall</td>\n",
              "      <td>0.931373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average Latency (ms)</td>\n",
              "      <td>3.769606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Average Tokens</td>\n",
              "      <td>5328.081081</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Metric  Ensemble_retrieval\n",
              "0  Average Context Precision            0.768539\n",
              "1     Average Context Recall            0.931373\n",
              "2       Average Latency (ms)            3.769606\n",
              "3             Average Tokens         5328.081081"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain_dataset = generate_answers(\"Ensemble_retrieval\", ensemble_retrieval_chain, ragas_testset)\n",
        "ensemble_retrieval_chain_evaluation = evaluate_ragas_results(ensemble_retrieval_chain_dataset)\n",
        "ensemble_retrieval_dataset = save_datasets(\"Ensemble_retrieval\",ensemble_retrieval_chain_dataset,ensemble_retrieval_chain_evaluation )\n",
        "get_averages(\"Ensemble_retrieval\", ensemble_retrieval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Semantic Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers generated\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a5bb6cb0d0043d8b70366104a88c22b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation complete\n",
            "Datasets merged and saved offline\n",
            "                      Metric  ParentDocument_retrieval\n",
            "0  Average Context Precision                  0.716682\n",
            "1     Average Context Recall                  0.972973\n",
            "2       Average Latency (ms)                  1.400724\n",
            "3             Average Tokens               2801.702703\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>ParentDocument_retrieval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Average Context Precision</td>\n",
              "      <td>0.716682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Average Context Recall</td>\n",
              "      <td>0.972973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average Latency (ms)</td>\n",
              "      <td>1.400724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Average Tokens</td>\n",
              "      <td>2801.702703</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Metric  ParentDocument_retrieval\n",
              "0  Average Context Precision                  0.716682\n",
              "1     Average Context Recall                  0.972973\n",
              "2       Average Latency (ms)                  1.400724\n",
              "3             Average Tokens               2801.702703"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "semantic_chunking_retrieval_chain_dataset = generate_answers(\"ParentDocument_retrieval\", semantic_retrieval_chain, ragas_testset)\n",
        "semantic_chunking_retrieval_chain_evaluation = evaluate_ragas_results(semantic_chunking_retrieval_chain_dataset)\n",
        "semantic_chunking_retrieval_dataset = save_datasets(\"ParentDocument_retrieval\",semantic_chunking_retrieval_chain_dataset,semantic_chunking_retrieval_chain_evaluation )\n",
        "get_averages(\"ParentDocument_retrieval\", semantic_chunking_retrieval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets summarize all of the scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  Dataset  Average Context Precision  Average Context Recall  Average Latency (ms)  Average Tokens\n",
            "          Naive Retrieval                   0.733476                0.936937              1.486473     3620.405405\n",
            "           BM25 Retrieval                   0.656907                0.783784              1.033972     1275.081081\n",
            "    Multi Query Retrieval                   0.747101                0.959459              3.323619     4465.189189\n",
            "Parent Document Retrieval                   0.756006                0.765766              1.311893      648.513514\n",
            "       Ensemble Retrieval                   0.768539                0.931373              3.769606     5328.081081\n",
            "        Semantic Chunking                   0.716682                0.972973              1.400724     2801.702703\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_averages(df):\n",
        "    averages = {\n",
        "        \"Average Context Precision\": df[\"context_precision\"].mean(),\n",
        "        \"Average Context Recall\": df[\"context_recall\"].mean(),\n",
        "        \"Average Latency (ms)\": df[\"latency\"].mean(),\n",
        "        \"Average Tokens\": df[\"tokens\"].mean()\n",
        "    }\n",
        "    return averages\n",
        "\n",
        "datasets = [\n",
        "    (naive_retrieval_dataset, \"Naive Retrieval\"),\n",
        "    (bm25_retrieval_dataset, \"BM25 Retrieval\"),\n",
        "    (multi_query_retrieval_dataset, \"Multi Query Retrieval\"),\n",
        "    (parent_document_retrieval_dataset, \"Parent Document Retrieval\"),\n",
        "    (ensemble_retrieval_dataset, \"Ensemble Retrieval\"),\n",
        "    (semantic_chunking_retrieval_dataset, \"Semantic Chunking\")\n",
        "]\n",
        "results = []\n",
        "for dataset, name in datasets:\n",
        "    avg_values = get_averages(dataset)\n",
        "    avg_values[\"Dataset\"] = name \n",
        "    results.append(avg_values)\n",
        "summary_df = pd.DataFrame(results)\n",
        "summary_df = summary_df[[\"Dataset\", \"Average Context Precision\", \"Average Context Recall\", \"Average Latency (ms)\", \"Average Tokens\"]]\n",
        "\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "summary_df.to_csv(\"summary_averages.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " #### Results and Analysis                 \n",
        "\n",
        "Average scores:  \n",
        "\n",
        "| Dataset                       | Average Context Precision | Average Context Recall | Average Latency (s) | Average Tokens | Total Latency | Total Tokens | LS Cost |\n",
        "|-------------------------------|---------------------------|------------------------|-----------------------|----------------|-----------|-----------|---------|\n",
        "| Naive Retrieval               | 0.733476                  | 0.936937               | 1.486473              | 3620.405405    | 198.13    | 133955    | 0.070   |\n",
        "| BM25 Retrieval                | 0.656907                  | 0.783784               | 1.033972              | 1275.08108     | 38.28     | 47178     | 0.026   |\n",
        "| Multi Query Retrieval         | 0.747101                  | 0.959459               | 3.323619              | 4465.189189    | 122.99    | 171764    | 0.091   |\n",
        "| Parent Document Retrieval     | 0.756006                  | 0.765766               | 1.311893              | 648.513514     | 48.55     | 23995     | 0.014   |\n",
        "| Ensemble Retrieval            | 0.768539                  | 0.931373               | 3.769606              | 5328.081081    | 139.49    | 203677    | 0.107   |\n",
        "| Semantic Chunking             | 0.716682                  | 0.972973               | 1.400724              | 2801.702703    | 51.84     | 103663    | 0.054   |\n",
        "\n",
        "Ranking 1 (best) - 6 (worst)\n",
        "\n",
        "| | Dataset                    | Precision Score | Recall Score | Latency Score | Tokens Score | Total Latency | Total Tokens | Cost $ |\n",
        "|-|----------------------------|-----------------|--------------|----------------|--------------|--------------|--------------|--------|\n",
        "| | Naive Retrieval            | 4               | 3            | 4              | 4            | 6            | 4            | 4      |\n",
        "| | BM25 Retrieval             | 6               | 5            | 1              | 2            | 1            | 2            | 2      |\n",
        "| | Multi Query Retrieval      | 3               | 2            | 4              | 5            | 4            | 5            | 5      |\n",
        "| | Parent Document Retrieval  | 2               | 6            | 2              | 1            | 2            | 1            | 1      |\n",
        "| | Ensemble Retrieval         | 1               | 4            | 6              | 6            | 5            | 6            | 6      |\n",
        "| | Semantic Chunking          | 5               | 1            | 3              | 3            | 3            | 3            | 3      |\n",
        " \n",
        "\n",
        "- Highest in Context Precision is Ensemble retrieval followed by Parent Document and Multi Query\n",
        "- Highest in Context Recall is Semantic Chunking followed closely by Multi Query and Naive\n",
        "- Lowest in Latency is BM25 followed by Parent Document and Semantic Chunking\n",
        "- Lowest in Average Tokens is Parent Document followed by BM25 and Semantic Chunking\n",
        "- Lowest Total Latency is BM25 followef by Parent Document and Semantic Chunking\n",
        "- Lowest Total Tokens is Parent Document followed by BM24 and Semantic Chunking\n",
        "- Lowest Total Cost is Parent Document followed by BM25 and Semantic Chunking\n",
        "\n",
        "Ensemble retrieval has the highest average context retrieval and a high context recall indicating it is effective at returning both relevant results and a good range of relevant documents.\n",
        "\n",
        "Semantic Chunking has the highest Context recall and a somewhat lower context precision indicating its ability to get a wide range of relevant documents but not so good at getting the most relevant results. It is in the middle of the results for latency and cost.\n",
        "\n",
        "BM25 has the lowest latency making it the fastest retrieval. Not surprising considering it is an efficient ranking algorithm. However BM25 does the worst in precision and second worst in recall. Its speed could be an asset in real time applications\n",
        "\n",
        "Multi Query has the second highest latency due to its strategy of using multiple queries. It is also the second most costliest. These higher latency and costs do help it somewhat in precision and recall.\n",
        "\n",
        "Parent Document retrieval has the second highest context precision but the lowest recall score. It will retrieve hightly relevant documents but will also miss some relevant ones. It has the second lowest latency and the lowest cost in tokens.\n",
        "\n",
        "Ensemble Retrieval has the highest latency (slowest) and highest cost in tokens. Since it uses the other techniques this is not surprising. What is surprising is that even using all of these different retrieval methods it does not do so well on precision and recall. \n",
        "\n",
        "If precision is a priority (returning the most relevant documents), then Ensemble or Parent Document retrieval may be best. Parent Documemt is also the cheapest retrieval mechanism and pretty quick. Ensemble on the other hand is slower and most expensive.\n",
        "\n",
        "If speed and cost are priorities then BM25 or Parent Document could be a good choice. BM25 comes with poor scores in precision or recall. Parent Document does well in precision.\n",
        "\n",
        "Summary\n",
        "\n",
        "- Ensemble Retrieval provides the best overall performance but at a high cost and latency.\n",
        "- BM25 is the most efficient method providing low costs and high speed but at the cost of precision and accuracy\n",
        "\n",
        "\n",
        "For this application, providing users the opportunity to ask questions about reviews of the John Wick movies, The Semantic Chunking method may be the best choice. It has the best recall score although a slightly lower precision and latency and cost in the lower half. This means it can provide relevant information both quickly and accurately. Its low cost adds to the benefits this retrieval method provides. If cost is not a concern then Multi Query may be a good option\n",
        "\n",
        "#### Winner ==> Semantic Chunking\n",
        "\n",
        "#### Runner Up ==> Multi Query\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
