{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Using cached ragas-0.1.20-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting numpy (from ragas)\n",
      "  Using cached numpy-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting datasets (from ragas)\n",
      "  Using cached datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting tiktoken (from ragas)\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain (from ragas)\n",
      "  Using cached langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3 (from ragas)\n",
      "  Using cached langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Using cached langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting langchain-openai (from ragas)\n",
      "  Using cached langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting openai>1 (from ragas)\n",
      "  Using cached openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pysbd>=0.3.4 (from ragas)\n",
      "  Using cached pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/rchrdgwr/anaconda3/envs/env_ragas/lib/python3.11/site-packages (from ragas) (1.6.0)\n",
      "Collecting appdirs (from ragas)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<0.3->ragas)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3->ragas)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.112 (from langchain-core<0.3->ragas)\n",
      "  Using cached langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/rchrdgwr/anaconda3/envs/env_ragas/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (24.1)\n",
      "Collecting pydantic<3,>=1 (from langchain-core<0.3->ragas)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3->ragas)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/rchrdgwr/anaconda3/envs/env_ragas/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (4.12.2)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>1->ragas)\n",
      "  Using cached anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>1->ragas)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>1->ragas)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>1->ragas)\n",
      "  Using cached jiter-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting sniffio (from openai>1->ragas)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai>1->ragas)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting filelock (from datasets->ragas)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->ragas)\n",
      "  Using cached pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->ragas)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting requests>=2.32.2 (from datasets->ragas)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets->ragas)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->ragas)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->ragas)\n",
      "  Using cached aiohttp-3.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.22.0 (from datasets->ragas)\n",
      "  Using cached huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->ragas)\n",
      "  Using cached SQLAlchemy-2.0.35-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain (from ragas)\n",
      "  Using cached langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->ragas)\n",
      "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting numpy (from ragas)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->ragas)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community (from ragas)\n",
      "  Using cached langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Using cached langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-openai (from ragas)\n",
      "  Using cached langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Using cached langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->ragas)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->ragas)\n",
      "  Using cached aiohappyeyeballs-2.4.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->ragas)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets->ragas)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->ragas)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->ragas)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets->ragas)\n",
      "  Using cached yarl-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai>1->ragas)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
      "  Using cached marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai>1->ragas)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>1->ragas)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3->ragas)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3->ragas)\n",
      "  Using cached orjson-3.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1->langchain-core<0.3->ragas)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=1->langchain-core<0.3->ragas)\n",
      "  Using cached pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets->ragas)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets->ragas)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain->ragas)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rchrdgwr/anaconda3/envs/env_ragas/lib/python3.11/site-packages (from pandas->datasets->ragas) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->ragas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->ragas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/rchrdgwr/anaconda3/envs/env_ragas/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached ragas-0.1.20-py3-none-any.whl (190 kB)\n",
      "Using cached langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
      "Using cached openai-1.50.2-py3-none-any.whl (382 kB)\n",
      "Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Using cached langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached langchain_community-0.2.17-py3-none-any.whl (2.3 MB)\n",
      "Using cached langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached aiohttp-3.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Using cached jiter-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.129-py3-none-any.whl (292 kB)\n",
      "Using cached pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached SQLAlchemy-2.0.35-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Using cached orjson-3.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached yarl-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: pytz, appdirs, xxhash, urllib3, tzdata, tqdm, tenacity, sniffio, regex, PyYAML, pysbd, pydantic-core, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, jiter, idna, h11, greenlet, fsspec, frozenlist, filelock, distro, dill, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, pyarrow, pandas, multiprocess, jsonpatch, httpcore, anyio, aiosignal, tiktoken, huggingface-hub, httpx, dataclasses-json, aiohttp, openai, langsmith, langchain-core, datasets, langchain-text-splitters, langchain-openai, langchain, langchain-community, ragas\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.2 aiohttp-3.10.8 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.0 appdirs-1.4.4 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.3.2 dataclasses-json-0.6.7 datasets-3.0.1 dill-0.3.8 distro-1.9.0 filelock-3.16.1 frozenlist-1.4.1 fsspec-2024.6.1 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 huggingface-hub-0.25.1 idna-3.10 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.16 langchain-community-0.2.17 langchain-core-0.2.41 langchain-openai-0.1.25 langchain-text-splitters-0.2.4 langsmith-0.1.129 marshmallow-3.22.0 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 numpy-1.26.4 openai-1.50.2 orjson-3.10.7 pandas-2.2.3 pyarrow-17.0.0 pydantic-2.9.2 pydantic-core-2.23.4 pysbd-0.3.4 pytz-2024.2 ragas-0.1.20 regex-2024.9.11 requests-2.32.3 sniffio-1.3.1 tenacity-8.5.0 tiktoken-0.7.0 tqdm-4.66.5 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-29 09:47:07--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19628 (19K) [text/plain]\n",
      "Saving to: ‘john_wick_1.csv’\n",
      "\n",
      "john_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-09-29 09:47:07 (8.06 MB/s) - ‘john_wick_1.csv’ saved [19628/19628]\n",
      "\n",
      "--2024-09-29 09:47:07--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14747 (14K) [text/plain]\n",
      "Saving to: ‘john_wick_2.csv’\n",
      "\n",
      "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-09-29 09:47:08 (7.60 MB/s) - ‘john_wick_2.csv’ saved [14747/14747]\n",
      "\n",
      "--2024-09-29 09:47:08--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13888 (14K) [text/plain]\n",
      "Saving to: ‘john_wick_3.csv’\n",
      "\n",
      "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-09-29 09:47:08 (7.53 MB/s) - ‘john_wick_3.csv’ saved [13888/13888]\n",
      "\n",
      "--2024-09-29 09:47:08--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15109 (15K) [text/plain]\n",
      "Saving to: ‘john_wick_4.csv’\n",
      "\n",
      "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-09-29 09:47:09 (74.3 MB/s) - ‘john_wick_4.csv’ saved [15109/15109]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "  loader = CSVLoader(\n",
    "      file_path=f\"john_wick_{i}.csv\",\n",
    "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
    "  )\n",
    "\n",
    "  movie_docs = loader.load()\n",
    "  for doc in movie_docs:\n",
    "\n",
    "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
    "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
    "\n",
    "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
    "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
    "\n",
    "    # newer movies have a more recent \"last_accessed_at\"\n",
    "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
    "\n",
    "  documents.extend(movie_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "599\n",
      "Average Content Length: 533.9\n",
      "Minimum Content Length: 29\n",
      "Maximum Content Length: 2440\n"
     ]
    }
   ],
   "source": [
    "documents[0]\n",
    "print(len(documents))\n",
    "content_1 = documents[0].page_content\n",
    "print(len(content_1))\n",
    "lengths = [len(doc.page_content) for doc in documents]\n",
    "average_length = sum(lengths) / len(lengths) if lengths else 0\n",
    "min_length = min(lengths) if lengths else 0\n",
    "max_length = max(lengths) if lengths else 0\n",
    "\n",
    "print(f\"Average Content Length: {average_length}\")\n",
    "print(f\"Minimum Content Length: {min_length}\")\n",
    "print(f\"Maximum Content Length: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the questions for evaluation\n",
    "\n",
    "We will create them once then store them for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "import pickle\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
    "file_path = 'ragas_testset.pkl'\n",
    "\n",
    "# load an existing ragas testset\n",
    "def load_ragas_testset_if_exists():\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                ragas_state = pickle.load(f)\n",
    "            print(f\"Ragas testset loaded from {file_path}\")\n",
    "            return ragas_state\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ragas testset: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No existing ragas tesetset found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Save the ragas testset\n",
    "def save_ragas_testset(testset):\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(testset, f)\n",
    "        print(f\"Ragas testset saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving ragas testset: {e}\")\n",
    "\n",
    "\n",
    "# create questions\n",
    "def create_questions_for_ragas(documents, num_questions=1):\n",
    "    generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    generator = TestsetGenerator.from_langchain(\n",
    "        generator_llm,\n",
    "        critic_llm,\n",
    "        embeddings\n",
    "    )\n",
    "    distributions = {\n",
    "        simple: 0.5,\n",
    "        multi_context: 0.4,\n",
    "        reasoning: 0.1\n",
    "    }\n",
    "    \n",
    "    testset = generator.generate_with_langchain_docs(documents, num_questions, distributions, with_debugging_logs=False)\n",
    "    save_ragas_testset(testset)\n",
    "    return testset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas testset loaded from ragas_testset.pkl\n"
     ]
    }
   ],
   "source": [
    "create_questions = False\n",
    "ragas_testset = None\n",
    "num_questions = 40\n",
    "if create_questions:\n",
    "    ragas_testset = create_questions_for_ragas(documents, num_questions)\n",
    "else:\n",
    "    ragas_testset = load_ragas_testset_if_exists()\n",
    "if ragas_testset:\n",
    "    ragas_testset.to_pandas()\n",
    "else:\n",
    "    print(\"No RAGAS testset found - need to create questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What makes John Wick stand out as a favorite r...</td>\n",
       "      <td>[: 22\\nReview: John Wick is one of my favourit...</td>\n",
       "      <td>John Wick stands out as a favorite recent year...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'john_wick_2.csv', 'row': 22, 'Rev...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some examples of classic internationa...</td>\n",
       "      <td>[: 11\\nReview: JOHN WICK is a rare example of ...</td>\n",
       "      <td>Some examples of classic international films s...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'john_wick_1.csv', 'row': 11, 'Rev...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the surprise hit movie starring Keanu...</td>\n",
       "      <td>[: 6\\nReview: In 2014, a Keanu Reeves revenge ...</td>\n",
       "      <td>John Wick</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'john_wick_2.csv', 'row': 6, 'Revi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who does John Wick face as he is called upon b...</td>\n",
       "      <td>[: 5\\nReview: Iosef's uncle still has John Wic...</td>\n",
       "      <td>John Wick faces deadly assassins, numerous kil...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'john_wick_2.csv', 'row': 5, 'Revi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the genre and main actor of the film \"...</td>\n",
       "      <td>[: 21\\nReview: John Wick is an action film wit...</td>\n",
       "      <td>The genre of the film 'John Wick' is action, a...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'john_wick_1.csv', 'row': 21, 'Rev...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What makes John Wick stand out as a favorite r...   \n",
       "1  What are some examples of classic internationa...   \n",
       "2  What was the surprise hit movie starring Keanu...   \n",
       "3  Who does John Wick face as he is called upon b...   \n",
       "4  What is the genre and main actor of the film \"...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [: 22\\nReview: John Wick is one of my favourit...   \n",
       "1  [: 11\\nReview: JOHN WICK is a rare example of ...   \n",
       "2  [: 6\\nReview: In 2014, a Keanu Reeves revenge ...   \n",
       "3  [: 5\\nReview: Iosef's uncle still has John Wic...   \n",
       "4  [: 21\\nReview: John Wick is an action film wit...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  John Wick stands out as a favorite recent year...         simple   \n",
       "1  Some examples of classic international films s...         simple   \n",
       "2                                          John Wick         simple   \n",
       "3  John Wick faces deadly assassins, numerous kil...         simple   \n",
       "4  The genre of the film 'John Wick' is action, a...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'john_wick_2.csv', 'row': 22, 'Rev...          True  \n",
       "1  [{'source': 'john_wick_1.csv', 'row': 11, 'Rev...          True  \n",
       "2  [{'source': 'john_wick_2.csv', 'row': 6, 'Revi...          True  \n",
       "3  [{'source': 'john_wick_2.csv', 'row': 5, 'Revi...          True  \n",
       "4  [{'source': 'john_wick_1.csv', 'row': 21, 'Rev...          True  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_testset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a RAG chain for evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def generate_answers(chain, testset):\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    questions = testset.to_pandas()[\"question\"].values.tolist()\n",
    "    ground_truths = testset.to_pandas()[\"ground_truth\"].values.tolist()\n",
    "\n",
    "    for question in questions:\n",
    "        answer = chain.invoke({\"question\" : question})\n",
    "        answers.append(answer[\"response\"])\n",
    "        contexts.append([context.page_content for context in answer[\"context\"]])\n",
    "    return Dataset.from_dict({\n",
    "        \"question\" : questions,\n",
    "        \"answer\" : answers,\n",
    "        \"contexts\" : contexts,\n",
    "        \"ground_truth\" : ground_truths\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
