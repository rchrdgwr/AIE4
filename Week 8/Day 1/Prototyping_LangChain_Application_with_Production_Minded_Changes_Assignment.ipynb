{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZsP-j7w3zcL"
      },
      "source": [
        "# Prototyping LangChain Application with Production Minded Changes\n",
        "\n",
        "For our first breakout room we'll be exploring how to set-up a LangChain LCEL chain in a way that takes advantage of all of the amazing out of the box production ready features it offers.\n",
        "\n",
        "We'll also explore `Caching` and what makes it an invaluable tool when transitioning to production environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpeN9ND0HKa0"
      },
      "source": [
        "## Task 1: Dependencies and Set-Up\n",
        "\n",
        "Let's get everything we need - we're going to use very specific versioning today to try to mitigate potential env. issues!\n",
        "\n",
        "> NOTE: Dependency issues are a large portion of what you're going to be tackling as you integrate new technology into your work - please keep in mind that one of the things you should be passively learning throughout this course is ways to mitigate dependency issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0P4IJUQF27jW"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_openai==0.2.0 langchain_community==0.3.0 langchain==0.3.0 pymupdf==1.24.10 qdrant-client==1.11.2 langchain_qdrant==0.1.4 langsmith==0.1.121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYcWLzrmHgDb"
      },
      "source": [
        "We'll need an OpenAI API Key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ8qfrFh_6ed",
        "outputId": "4fb1a16f-1f71-4d0a-aad4-dd0d0917abc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz2DUDuHiSO"
      },
      "source": [
        "And the LangSmith set-up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZX5zowCh-q",
        "outputId": "565c588a-a865-4b86-d5ca-986f35153000"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM Week 8 Assignment 1 - {uuid.uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmwNTziKHrQm"
      },
      "source": [
        "Let's verify our project so we can leverage it in LangSmith later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6GZmkVkFcHq",
        "outputId": "f4c0fdb3-24ea-429a-fa8c-23556cb7c3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIM Week 8 Assignment 1 - e3c95832\n"
          ]
        }
      ],
      "source": [
        "print(os.environ[\"LANGCHAIN_PROJECT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un_ppfaAHv1J"
      },
      "source": [
        "## Task 2: Setting up RAG With Production in Mind\n",
        "\n",
        "This is the most crucial step in the process - in order to take advantage of:\n",
        "\n",
        "- Asyncronous requests\n",
        "- Parallel Execution in Chains\n",
        "- And more...\n",
        "\n",
        "You must...use LCEL. These benefits are provided out of the box and largely optimized behind the scenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGi-db23JMAL"
      },
      "source": [
        "### Building our RAG Components: Retriever\n",
        "\n",
        "We'll start by building some familiar components - and showcase how they automatically scale to production features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvbT3HSDJemE"
      },
      "source": [
        "Please upload a PDF file to use in this example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pymupdf\n",
        "!pip install -q ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to save an uploaded file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_uploaded_file(upload_widget, folder=\"data\"):\n",
        "    if upload_widget.value:\n",
        "        # Get the uploaded file as a dictionary (from the tuple)\n",
        "        uploaded_file = upload_widget.value[0]  # Access the first item in the tuple\n",
        "        pdf_data = uploaded_file['content']  # Extract the content (which is a memory object)\n",
        "        pdf_name = uploaded_file['name']  # Extract the file name\n",
        "        \n",
        "        # Ensure the target folder exists\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "        \n",
        "        # Define full path for the file\n",
        "        file_path = os.path.join(folder, pdf_name)\n",
        "        \n",
        "        # Save the file\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(pdf_data.tobytes())  # Convert memory content to bytes and write\n",
        "        \n",
        "        print(f\"File saved as: {file_path}\")\n",
        "        \n",
        "        return file_path\n",
        "    else:\n",
        "        print(\"No file uploaded\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select a file from the local drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dvYczNeY91Hn",
        "outputId": "c711c29b-e388-4d32-a763-f4504244eef2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c0ff75685724721bf5d656bb365c5e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value=(), accept='.pdf', description='Upload')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "upload_widget = widgets.FileUpload(accept='.pdf', multiple=False)  # To only allow PDF uploads\n",
        "display(upload_widget)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No file uploaded\n"
          ]
        }
      ],
      "source": [
        "\n",
        "file_path = save_uploaded_file(upload_widget)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kucGy3f0Jhdi"
      },
      "source": [
        "We'll define our chunking strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "G-DNvNFd8je5"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_zRRNcLKCZh"
      },
      "source": [
        "We'll chunk our uploaded PDF file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOh6w9ud-ff6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "Loader = PyMuPDFLoader\n",
        "loader = Loader(file_path)\n",
        "documents = loader.load()\n",
        "docs = text_splitter.split_documents(documents)\n",
        "for i, doc in enumerate(docs):\n",
        "    doc.metadata[\"source\"] = f\"source_{i}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4XLeqJMKGdQ"
      },
      "source": [
        "#### QDrant Vector Database - Cache Backed Embeddings\n",
        "\n",
        "The process of embedding is typically a very time consuming one - we must, for ever single vector in our VDB as well as query:\n",
        "\n",
        "1. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "2. Wait for processing\n",
        "3. Receive response\n",
        "\n",
        "This process costs time, and money - and occurs *every single time a document gets converted into a vector representation*.\n",
        "\n",
        "Instead, what if we:\n",
        "\n",
        "1. Set up a cache that can hold our vectors and embeddings (similar to, or in some cases literally a vector database)\n",
        "2. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "3. Check the cache to see if we've already converted this text before.\n",
        "  - If we have: Return the vector representation\n",
        "  - Else: Wait for processing and proceed\n",
        "4. Store the text that was converted alongside its vector representation in a cache of some kind.\n",
        "5. Return the vector representation\n",
        "\n",
        "Notice that we can shortcut some instances of \"Wait for processing and proceed\".\n",
        "\n",
        "Let's see how this is implemented in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dzPUTCua98b2"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "# Typical Embedding Model\n",
        "core_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Typical QDrant Client Set-up\n",
        "collection_name = f\"pdf_to_parse_{uuid.uuid4()}\"\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "# Adding cache!\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings, store, namespace=core_embeddings.model\n",
        ")\n",
        "\n",
        "# Typical QDrant Vector Store Set-up\n",
        "vectorstore = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding=cached_embedder)\n",
        "vectorstore.add_documents(docs)\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVZGvmNYLomp"
      },
      "source": [
        "##### ‚ùì Question #1:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "\n",
        "> NOTE: There is no single correct answer here!\n",
        "\n",
        "#### ! Answer #1:\n",
        "\n",
        "- Cache is stored locally - this could counsume a lot of disk space for very large vector databases\n",
        "    - the document pdf consumes 41 MB\n",
        "    - each text embedding takes up 34kb. There are 589 files. They consume 20.7 MB of disk space \n",
        "- Changes to an existing document will cause new cached embeddings to be created without the removal of the older cached embeddings\n",
        "- Require some kind of cache purging to manage the cache properly or may need to manually monitor and manage it\n",
        "- Changing to a new embedding model or possibly switching to a new version could cause all cached embeddings to be invalid\n",
        "- This looks like a single cache store - I dont see collection name associated, so wondering if this shared cache could be problematic in serving up the wrong information\n",
        "- Disk i/o can become a bottleneck\n",
        "- The cache relies on an exact match for an embedding so could miss synonyms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAOhyb3L9iD"
      },
      "source": [
        "##### üèóÔ∏è Activity #1:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can embed the same document twice while timing the embedding process\n",
        "\n",
        "Lets get a new document that we will then embed twice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Determine Time Differential Caused by Emnedding Caching\n",
        "\n",
        "Create functions to time the embedding\n",
        "- create_documents - chunks the document\n",
        "- create_embeddings - either creates embeddins or identifies them in cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def create_documents(file_path):\n",
        "    loader = Loader(file_path)\n",
        "    documents = loader.load()\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    for i, doc in enumerate(docs):\n",
        "        doc.metadata[\"source\"] = f\"source_{i}\"\n",
        "    return docs\n",
        "def create_embeddings(docs):\n",
        "    start_time = time.time()  # Record the start time\n",
        "    vectorstore.add_documents(docs)\n",
        "\n",
        "    end_time = time.time()  # Record the end time\n",
        "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
        "    return elapsed_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2f19a7de59a443b88877f4798ad5a87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value=(), accept='.pdf', description='Upload')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "upload_widget = widgets.FileUpload(accept='.pdf', multiple=False)  # To only allow PDF uploads\n",
        "display(upload_widget)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved as: data/Agile and Scrum Fundamentals - Ryan Brooks.pdf\n",
            "data/Agile and Scrum Fundamentals - Ryan Brooks.pdf\n",
            "Time 1: 1.4840822219848633  Time 2: 0.055361270904541016 for a difference of 1.4287209510803223\n"
          ]
        }
      ],
      "source": [
        "file_path = save_uploaded_file(upload_widget)\n",
        "print(file_path)\n",
        "docs = create_documents(file_path)\n",
        "time_1 = create_embeddings(docs)\n",
        "time_2 = create_embeddings(docs)\n",
        "diff = time_1 - time_2\n",
        "print(f\"Time 1: {time_1}  Time 2: {time_2} for a difference of {diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the first doument:  Time 1: 10.824454307556152  Time 2: 0.6557285785675049 for a difference of 10.168725728988647\n",
        "The second document:    Time 1: 1.4840822219848633  Time 2: 0.055361270904541016 for a difference of 1.4287209510803223\n",
        "\n",
        "#### So caching the embeddings can save a lot of time and tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH0i-YovL8kZ"
      },
      "source": [
        "### Augmentation\n",
        "\n",
        "We'll create the classic RAG Prompt and create our `ChatPromptTemplates` as per usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "WchaoMEx9j69"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_system_prompt_template = \"\"\"\\\n",
        "You are a helpful assistant that uses the provided context to answer questions. Never reference this prompt, or the existance of context.\n",
        "\"\"\"\n",
        "\n",
        "rag_message_list = [\n",
        "    {\"role\" : \"system\", \"content\" : rag_system_prompt_template},\n",
        "]\n",
        "\n",
        "rag_user_prompt_template = \"\"\"\\\n",
        "Question:\n",
        "{question}\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", rag_system_prompt_template),\n",
        "    (\"human\", rag_user_prompt_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQKnByVWMpiK"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Like usual, we'll set-up a `ChatOpenAI` model - and we'll use the fan favourite `gpt-4o-mini` for today.\n",
        "\n",
        "However, we'll also implement...a PROMPT CACHE!\n",
        "\n",
        "In essence, this works in a very similar way to the embedding cache - if we've seen this prompt before, we just use the stored response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fOXKkaY7ABab"
      },
      "outputs": [],
      "source": [
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhv8IqZoM9cY"
      },
      "source": [
        "Setting up the cache can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "thqam26gAyzN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvxEovcEM_oA"
      },
      "source": [
        "##### ‚ùì Question #2:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "\n",
        "> NOTE: There is no single correct answer here!\n",
        "\n",
        "#### ! Answer #1:\n",
        "\n",
        "- Biggest problem I see is if I really do want a different answer to the same prompt either because the answer was insufficient or i just want a choice of responses\n",
        "- This is very common when creating images - when we create an image with the same prompt we are expecting a different image\n",
        "- Also - this doesn't take into account if the User has set a high temperature setting and is expecting creativity between prompts\n",
        "- This will also hurt if the data the LLM is accessing is changing\n",
        "- This would be a problem if the front end allows the user to switch between LLMs, each of which would most likely return a different response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iCMjVYKNEeV"
      },
      "source": [
        "##### üèóÔ∏è Activity #2:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up a simple prompt and chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "QT5GfmsHNFqP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Provide a short concise answer to the question based on your knowledge\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "experimental_chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])\n",
        "\n",
        "chain = experimental_chat_prompt | chat_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Once in a small village, a young girl named Lila discovered a hidden garden filled with vibrant flowers that only bloomed under the moonlight. Each night, she would sneak away to explore this magical place, where she could hear the whispers of the flowers sharing secrets of the universe. One night, she met a wise old owl who told her that the garden was a sanctuary for lost dreams. Inspired, Lila decided to gather the dreams of her villagers and plant them in the garden. As the flowers blossomed, the villagers found renewed hope and purpose, transforming their lives. Lila learned that dreams, when nurtured, could create beauty and change in the world.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 28, 'total_tokens': 163, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-6be2c4c3-e40c-45a8-a0b5-152d2d787be8-0', usage_metadata={'input_tokens': 28, 'output_tokens': 135, 'total_tokens': 163})"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"content\": \"Tell me a story\"})\n",
        "chain.invoke({\"content\": \"Tell me a different story\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK - lets create a generic timer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def time_invoke(message):\n",
        "    start_time = time.time()  # Record the start time\n",
        "    result = chain.invoke({\"content\": message})\n",
        "    end_time = time.time()  # Record the end time\n",
        "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
        "\n",
        "    return {\"result\": result, \"elapsed_time\": elapsed_time}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets time some prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell me a story about London\n",
            "Tell me a story about London\n",
            "Who is Jim Croce?\n",
            "Who is Jim Croce?\n",
            "How do you make French bread\n",
            "How do you make French bread\n",
            "Message: Tell me a story about London\n",
            "Message: {'result': AIMessage(content='Once upon a time in London, a young artist named Clara roamed the bustling streets, sketching the iconic landmarks. One chilly autumn day, she stumbled upon an old, hidden bookstore in an alley. Inside, she found a dusty, leather-bound journal filled with stories of the city‚Äôs past. Intrigued, Clara took it home, and as she read, the characters came to life, leading her on adventures through the foggy streets of Victorian London.\\n\\nInspired, Clara began to paint the scenes from the journal, blending the past with her present. Her artwork caught the eye of a local gallery, and soon her exhibition, \"London Through Time,\" became a sensation. People flocked from all over to see the city through Clara‚Äôs eyes.\\n\\nAs she stood at her opening night, surrounded by her paintings and the echoes of the stories that had inspired her, Clara realized that London was not just a city of landmarks but a tapestry of stories waiting to be told. And in that moment, she knew she had found her place in the vibrant heart of London.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 214, 'prompt_tokens': 29, 'total_tokens': 243, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-50b49083-2fc0-4850-ae01-602a883749bb-0', usage_metadata={'input_tokens': 29, 'output_tokens': 214, 'total_tokens': 243}), 'elapsed_time': 2.904071569442749}\n",
            "Message: {'result': AIMessage(content='Once upon a time in London, a young artist named Clara roamed the bustling streets, sketching the iconic landmarks. One chilly autumn day, she stumbled upon an old, hidden bookstore in an alley. Inside, she found a dusty, leather-bound journal filled with stories of the city‚Äôs past. Intrigued, Clara took it home, and as she read, the characters came to life, leading her on adventures through the foggy streets of Victorian London.\\n\\nInspired, Clara began to paint the scenes from the journal, blending the past with her present. Her artwork caught the eye of a local gallery, and soon her exhibition, \"London Through Time,\" became a sensation. People flocked from all over to see the city through Clara‚Äôs eyes.\\n\\nAs she stood at her opening night, surrounded by her paintings and the echoes of the stories that had inspired her, Clara realized that London was not just a city of landmarks but a tapestry of stories waiting to be told. And in that moment, she knew she had found her place in the vibrant heart of London.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 214, 'prompt_tokens': 29, 'total_tokens': 243, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-50b49083-2fc0-4850-ae01-602a883749bb-0', usage_metadata={'input_tokens': 29, 'output_tokens': 214, 'total_tokens': 243}), 'elapsed_time': 0.002847433090209961}\n",
            "Time difference: 2.901224136352539 seconds\n",
            "\n",
            "Message: Who is Jim Croce?\n",
            "Message: {'result': AIMessage(content='Jim Croce was an American singer-songwriter known for his folk and rock music in the 1970s. He gained fame for hits like \"Bad, Bad Leroy Brown\" and \"Time in a Bottle.\" Croce\\'s storytelling style and emotive lyrics resonated with audiences, but his career was tragically cut short when he died in a plane crash in 1973.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 29, 'total_tokens': 105, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-bd2fe22c-c285-4ed1-adf2-966e602322c3-0', usage_metadata={'input_tokens': 29, 'output_tokens': 76, 'total_tokens': 105}), 'elapsed_time': 6.701268911361694}\n",
            "Message: {'result': AIMessage(content='Jim Croce was an American singer-songwriter known for his folk and rock music in the 1970s. He gained fame for hits like \"Bad, Bad Leroy Brown\" and \"Time in a Bottle.\" Croce\\'s storytelling style and emotive lyrics resonated with audiences, but his career was tragically cut short when he died in a plane crash in 1973.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 29, 'total_tokens': 105, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-bd2fe22c-c285-4ed1-adf2-966e602322c3-0', usage_metadata={'input_tokens': 29, 'output_tokens': 76, 'total_tokens': 105}), 'elapsed_time': 0.002756357192993164}\n",
            "Time difference: 6.698512554168701 seconds\n",
            "\n",
            "Message: How do you make French bread\n",
            "Message: {'result': AIMessage(content='To make French bread, follow these steps:\\n\\n1. **Ingredients**: Combine 4 cups of all-purpose flour, 1 ¬Ω cups of warm water, 2 teaspoons of salt, and 2 teaspoons of active dry yeast.\\n\\n2. **Mix**: Dissolve yeast in warm water, then mix in salt and flour gradually until a dough forms.\\n\\n3. **Knead**: Knead the dough on a floured surface for about 10 minutes until smooth.\\n\\n4. **First Rise**: Place in a greased bowl, cover, and let it rise for 1-2 hours until doubled in size.\\n\\n5. **Shape**: Punch down the dough, then shape it into loaves or baguettes.\\n\\n6. **Second Rise**: Place on a baking sheet, cover, and let rise for another 30-45 minutes.\\n\\n7. **Preheat Oven**: Preheat your oven to 450¬∞F (230¬∞C). Optionally, place a pan of water in the oven for steam.\\n\\n8. **Bake**: Slash the tops of the loaves and bake for 20-25 minutes until golden brown and hollow-sounding when tapped.\\n\\n9. **Cool**: Let the bread cool on a wire rack before slicing.\\n\\nEnjoy your homemade French bread!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 264, 'prompt_tokens': 29, 'total_tokens': 293, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-944d3a50-4aff-4807-ab10-2fb5608c0fda-0', usage_metadata={'input_tokens': 29, 'output_tokens': 264, 'total_tokens': 293}), 'elapsed_time': 4.863785743713379}\n",
            "Message: {'result': AIMessage(content='To make French bread, follow these steps:\\n\\n1. **Ingredients**: Combine 4 cups of all-purpose flour, 1 ¬Ω cups of warm water, 2 teaspoons of salt, and 2 teaspoons of active dry yeast.\\n\\n2. **Mix**: Dissolve yeast in warm water, then mix in salt and flour gradually until a dough forms.\\n\\n3. **Knead**: Knead the dough on a floured surface for about 10 minutes until smooth.\\n\\n4. **First Rise**: Place in a greased bowl, cover, and let it rise for 1-2 hours until doubled in size.\\n\\n5. **Shape**: Punch down the dough, then shape it into loaves or baguettes.\\n\\n6. **Second Rise**: Place on a baking sheet, cover, and let rise for another 30-45 minutes.\\n\\n7. **Preheat Oven**: Preheat your oven to 450¬∞F (230¬∞C). Optionally, place a pan of water in the oven for steam.\\n\\n8. **Bake**: Slash the tops of the loaves and bake for 20-25 minutes until golden brown and hollow-sounding when tapped.\\n\\n9. **Cool**: Let the bread cool on a wire rack before slicing.\\n\\nEnjoy your homemade French bread!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 264, 'prompt_tokens': 29, 'total_tokens': 293, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-944d3a50-4aff-4807-ab10-2fb5608c0fda-0', usage_metadata={'input_tokens': 29, 'output_tokens': 264, 'total_tokens': 293}), 'elapsed_time': 0.002827882766723633}\n",
            "Time difference: 4.860957860946655 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "messages = [\"Tell me a story about London\", \"Who is Jim Croce?\", \"How do you make French bread\"]\n",
        "responses = []\n",
        "for message in messages:\n",
        "\n",
        "    response_1 = time_invoke(message)\n",
        "    response_2 = time_invoke(message)\n",
        "    elapsed_1 = response_1[\"elapsed_time\"]\n",
        "    elapsed_2 = response_2[\"elapsed_time\"]\n",
        "    time_difference = abs(elapsed_1 - elapsed_2)\n",
        "\n",
        "    comparison = {\n",
        "        \"message\": message,\n",
        "        \"response_1\": response_1,\n",
        "        \"response_2\": response_2,\n",
        "        \"first_time\": elapsed_1,\n",
        "        \"second_time\": elapsed_2,\n",
        "        \"time_difference\": time_difference\n",
        "    }\n",
        "    \n",
        "    responses.append(comparison)\n",
        "\n",
        "for response in responses:\n",
        "    print(f\"Message: {response['message']}\")\n",
        "    print(f\"Message: {response['response_1']}\")\n",
        "    print(f\"Message: {response['response_2']}\")\n",
        "    print(f\"Time difference: {response['time_difference']} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message: Tell me a story about London, Time Difference: 2.901224136352539 seconds\n",
            "Message: Who is Jim Croce?, Time Difference: 6.698512554168701 seconds\n",
            "Message: How do you make French bread, Time Difference: 4.860957860946655 seconds\n"
          ]
        }
      ],
      "source": [
        "for entry in responses:\n",
        "    print(f\"Message: {entry['message']}, Time Difference: {entry['time_difference']} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results: \n",
        "\n",
        "- Tell me a story about London, Time Difference: 2.901224136352539 seconds\n",
        "- Who is Jim Croce?, Time Difference: 6.698512554168701 seconds\n",
        "- How do you make French bread, Time Difference: 4.860957860946655 seconds\n",
        "\n",
        "#### So using a Prompt Cache can save a lot of time and tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyPnNWb9NH7W"
      },
      "source": [
        "## Task 3: RAG LCEL Chain\n",
        "\n",
        "We'll also set-up our typical RAG chain using LCEL.\n",
        "\n",
        "However, this time: We'll specifically call out that the `context` and `question` halves of the first \"link\" in the chain are executed *in parallel* by default!\n",
        "\n",
        "Thanks, LCEL!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "3JNvSsx_CEtI"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | chat_prompt | chat_model\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx--wVctNdGa"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43uQegbnDQKP",
        "outputId": "a9ff032b-4eb2-4f5f-f456-1fc6aa24aaec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='1. The document is titled \"Agile and Scrum Fundamentals.\"\\n2. It is authored by Ryan Brooks.\\n3. The document is in PDF format, specifically version 1.5.\\n4. The document was created using Microsoft PowerPoint 2013.\\n5. The total number of pages is 80.\\n6. The current page referenced is page 57.\\n7. The document\\'s creation date is May 15, 2020.\\n8. The last modification date is also May 15, 2020.\\n9. The document includes information on Agile and Scrum methodologies.\\n10. It uses a presentation format, indicating a structured format for delivering content.\\n11. Page 37 features a comparison of frameworks versus detailed manuals.\\n12. The document appears to involve discussions on Test-Driven Development (TDD).\\n13. It mentions \"Swarming\" as a concept related to Agile practices.\\n14. The document lists steps in a process including \"Story Kickoff!\" and \"Write Accept Tests.\"\\n15. Unit testing is emphasized as part of the Agile process.\\n16. It instructs on writing documentation (\"Write Doc\").\\n17. Testing and integration processes are included in the workflow.\\n18. The document outlines a \"One-Piece Continuous Flow\" concept.\\n19. The author is Ryan Brooks, suggesting a focus on practical Agile applications.\\n20. The document is part of a collection identified by a unique ID.\\n21. The metadata includes keywords, although none are specified.\\n22. The document does not have a defined subject listed in the metadata.\\n23. The page content includes brief bullet points, indicating a concise presentation style.\\n24. There is a mention of \"Done!\" implying a completion stage in the Agile process.\\n25. The workflow seems to be iterative, following Agile principles.\\n26. The document is part of a larger collection of Agile resources.\\n27. The document may be used for training or educational purposes.\\n28. It highlights the importance of collaboration in Agile teams.\\n29. The reference to ‚ÄúPO Review‚Äù suggests involvement of a Product Owner in the process.\\n30. The document likely targets individuals or teams interested in Agile methodologies.\\n31. The document structure is likely designed for easy navigation and readability.\\n32. The phrase \"Next‚Ä¶\" indicates a forward-looking approach in the Agile process.\\n33. The content suggests a focus on practical application rather than theoretical concepts.\\n34. The mention of \"Int and Test\" may refer to integration and testing phases.\\n35. The document could be a resource for Agile coaches or Scrum Masters.\\n36. The design of the document implies a focus on visual learning.\\n37. The document likely contains examples or case studies related to Agile.\\n38. The author may have experience in Agile coaching or software development.\\n39. The emphasis on testing indicates a quality assurance perspective in Agile.\\n40. The document may include diagrams or visual aids to support understanding.\\n41. The creation tool suggests familiarity with presentation software.\\n42. The document\\'s title indicates it is foundational, likely for beginners in Agile.\\n43. The process outlined reflects common practices in software development.\\n44. The document may be used for workshops or interactive sessions.\\n45. It reflects modern practices in software engineering.\\n46. The document may be referenced in Agile certification programs.\\n47. The content is likely to evolve with Agile practices over time.\\n48. The mention of specific Agile roles suggests a focus on team dynamics.\\n49. The concise nature of the content aligns with Agile principles of simplicity.\\n50. The document serves as a guide for implementing Agile methodologies effectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 714, 'prompt_tokens': 1078, 'total_tokens': 1792, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-636e8bcf-2a54-487c-9715-ec23d688dccb-0', usage_metadata={'input_tokens': 1078, 'output_tokens': 714, 'total_tokens': 1792})"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"Write 50 things about this document!\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tYAvHrJNecy"
      },
      "source": [
        "##### üèóÔ∏è Activity #3:\n",
        "\n",
        "Show, through LangSmith, the different between a trace that is leveraging cache-backed embeddings and LLM calls - and one that isn't.\n",
        "\n",
        "Post screenshots in the notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Langsmith Trace With Parallel Steps\n",
        "\n",
        "![image](images/langsmith_parallel_2.jpg)\n",
        "![image](images/langsmith_parallel.jpg)\n",
        "\n",
        "The images above represent the request for \"Write 50 things about this document!\" which used the retrieval_augmented_qa_chain.\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | chat_prompt | chat_model\n",
        "    )\n",
        "\n",
        "\n",
        "This chain had 2 steps in parallel:\n",
        "- get the question and retrieve documents for the context by the retriever\n",
        "- extract of question from the input dictionary\n",
        "\n",
        "In LangSmith theis parallel processing is represented by RunnableParallel<context, question> which shows the steps run in parallel:\n",
        "- map:key:context including the VectorStoreRetriever\n",
        "- RunnableLambda\n",
        "\n",
        "The diagrams below show a simpler chain from a previous run that has no parallel steps:\n",
        "\n",
        "![image](images/langsmith_not_parallel.jpg)\n",
        "![image](images/langsmith_not_parallel_2.jpg)\n",
        "\n",
        "\n",
        "These diagrams show there is no parallel step - ie no RunnableParallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
