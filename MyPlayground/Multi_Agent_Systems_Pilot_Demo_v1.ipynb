{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IDUnpEEl-L_F"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Agent Workflows + RAG - LangGraph\n",
        "\n",
        "Today we'll be looking at an example of a Multi-Agent workflow that's powered by LangGraph, LCEL, and more!\n",
        "\n",
        "We're going to be, more specifically, looking at a \"heirarchical agent teams\" from the [AutoGen: Enabling Next-Gen LLM\n",
        "Applications via Multi-Agent Conversation](https://arxiv.org/pdf/2308.08155) paper.\n",
        "\n",
        "> NOTE: We'll be following along with the official LangGraph implementation very closely, which you can find [here](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb), with some minor modifications and extensions to showcase just how straightforward it is to modify LangGraph implementations to suit your own needs!\n",
        "\n"
      ],
      "metadata": {
        "id": "KxpWDFG11o3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll start, as we normally do, by grabbing our dependencies.\n",
        "\n",
        "We'll be using LangChain and LangGraph to power our application, so let's start by grabbing those!"
      ],
      "metadata": {
        "id": "mx3oaVoX5cA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs6HUTgecbzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77a3350-b0ea-4e9e-9b48-2dba08cd157a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m849.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langgraph langchain langchain_openai langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to be showing a simple RAG chain as part of our LangGraph - and so we'll need specific dependencies for that as well!"
      ],
      "metadata": {
        "id": "BMzWFUc25oqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU --disable-pip-version-check qdrant-client pymupdf tiktoken"
      ],
      "metadata": {
        "id": "qEUBCOdukjwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7250603-7772-4e8f-9f5d-b79fb679990d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.52.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.27.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we'll be relying on OpenAI's suite of models to power our agents today, we'll want to provide our OpenAI API Key.\n",
        "\n",
        "We're also going to be using the Tavily search tool - so we'll want to provide that API key as well!\n",
        "\n",
        "Instruction for how to obtain these API keys can be found:\n",
        "\n",
        "1. [OpenAI API Key](https://platform.openai.com/docs/quickstart#:~:text=Account%20setup,not%20share%20it%20with%20anyone.)\n",
        "2. [Tavily API Key](https://docs.tavily.com/docs/tavily-api/introduction#:~:text=Sign%20Up%3A%20Begin%20by%20signing,in%20our%20interactive%20API%20playground.)\n",
        "\n"
      ],
      "metadata": {
        "id": "Zpv2MWqu5vS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h30OjkLfeR2Y",
        "outputId": "80bc2e5c-775e-421c-a796-eb27f5de866d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n",
            "TAVILY_API_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple LCEL RAG\n",
        "\n",
        "Now that we have our dependencies set-up - let's create a simple RAG chain that works over a single PDF.\n",
        "\n",
        "> NOTE: While this particular example is very straight forward - you can \"plug in\" any complexity of chain you desire as a node in a LangGraph."
      ],
      "metadata": {
        "id": "M_LD7rwT6PbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval\n",
        "\n",
        "The 'R' in 'RAG' - this is, at this point, fairly straightforward!"
      ],
      "metadata": {
        "id": "JY7T5kxJ6jGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Collection and Processing\n",
        "\n",
        "A classic first step, at this point, let's grab our desired document!"
      ],
      "metadata": {
        "id": "DGuPxSCk7Ztz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = PyMuPDFLoader(\"https://skybrary.aero/sites/default/files/bookshelf/3177.pdf\").load()"
      ],
      "metadata": {
        "id": "LfuoEYRCln3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can chunk it down to size!"
      ],
      "metadata": {
        "id": "r_t_F1zG6vXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-4\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "5R7A_z8CgL79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've successfully split our single PDF into..."
      ],
      "metadata": {
        "id": "lGE-VuMc7AKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgYBHsdWmLvW",
        "outputId": "dda26b48-ec04-4ae8-9150-204fd1ff4f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "412"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "documents!"
      ],
      "metadata": {
        "id": "lxaKmmyh7DHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding Model and Vector Store\n",
        "\n",
        "Now that we have our chunked document - lets create a vector store, which will first require us to create an embedding model to get the vector representations of our text!\n",
        "\n",
        "We'll use OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) model - as it's cheap, and performant."
      ],
      "metadata": {
        "id": "cGWs7KTd7QPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "xLIWMMZCmfrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our QDrant backed vector store!"
      ],
      "metadata": {
        "id": "lTEi7Ww573sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    split_chunks,\n",
        "    embedding_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"pilot_demo_evidence_based\",\n",
        ")"
      ],
      "metadata": {
        "id": "Xct51f8omVAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure we can access it as a retriever."
      ],
      "metadata": {
        "id": "wzGq6o4s79Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "OTnQZbWymi4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented\n",
        "\n",
        "Now that we have our retrieval process set-up, we need to set up our \"augmentation\" process - AKA a prompt template."
      ],
      "metadata": {
        "id": "aU8qSrMS7_D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{question}\n",
        "\n",
        "You are a helpful assistant. Use the available context to answer the question. If you can't answer the question, say you don't know.\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ],
      "metadata": {
        "id": "lezTN0zCmk46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation\n",
        "\n",
        "Last, but certainly not least, let's put the 'G' in 'RAG' by adding our generator - in this case, we can rely on OpenAI's [`gpt-3.5-turbo`](https://platform.openai.com/docs/models/gpt-3-5-turbo) model!"
      ],
      "metadata": {
        "id": "Y9fa63nM7IKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "AwEi29-Jo3a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG - Retrieval Augmented Generation\n",
        "\n",
        "All that's left to do is combine our R, A, and G into a single chain - and we're off!"
      ],
      "metadata": {
        "id": "qO-ZC0T98XJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "evidence_based_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | openai_chat_model | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "nlOJrPm_oT3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this out and make sure it works."
      ],
      "metadata": {
        "id": "qiWrbXpu8ggz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_based_rag_chain.invoke({\"question\" : \"Define 'situational awareness'.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "gJhFlW32pBPe",
        "outputId": "bf27b206-d72c-4502-ec21-73ba0932d64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided context, situational awareness is defined as:\\n\\n\"Perceiving and comprehending all of the relevant information available and anticipating what could happen that may affect the operation.\"\\n\\nThis involves:\\n- Identifying and assessing accurately the state of the aircraft and its systems.\\n- Identifying and assessing accurately the aircraft’s vertical and lateral position and its anticipated flight path.\\n- Identifying and assessing accurately the general environment as it may affect the operation.\\n- Keeping track of time and fuel.\\n- Maintaining awareness of the people involved in or affected by the operation and their capacity to perform as expected.\\n- Anticipating accurately what could happen, planning, and staying ahead of the situation.\\n- Developing effective contingency plans based upon potential threats.\\n- Identifying and managing threats to the safety of the aircraft and people.\\n- Recognizing and effectively responding to indications of reduced situational awareness.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Limitation\n",
        "\n",
        "Notice how we're hard-coding our data, while this is simply meant to be an illustrative example - you could easily extend this to work with any provied paper or document in order to have a more dynamic system.\n",
        "\n",
        "For now, we'll stick with this single hard-coded example in order to keep complexity down in an already very long notebook!"
      ],
      "metadata": {
        "id": "gReMizYk8qd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple LCEL RAG - Air Force Handbook\n",
        "\n",
        "Now that we have our RAG for 'Manual of Evidence-based Training', we're going to create a separate RAG pipeline for the 'Air Force Handbook 1', which will help our teams understand the best methods of simulating Airmen."
      ],
      "metadata": {
        "id": "VfoQNT8GSucI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "handbook_docs = PyMuPDFLoader(\"https://static.e-publishing.af.mil/production/1/af_a1/publication/afh1/afh1.pdf\").load()"
      ],
      "metadata": {
        "id": "SQMUXvezSt-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handbook_split_chunks = text_splitter.split_documents(handbook_docs)"
      ],
      "metadata": {
        "id": "ESXTY27MTRdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(handbook_split_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzWRJFHlTX8p",
        "outputId": "2cb98ceb-f806-4410-ca2c-fb83b6c10bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1563"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "handbook_qdrant_vectorstore = Qdrant.from_documents(\n",
        "    split_chunks,\n",
        "    embedding_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"pilot_demo_airforce_handbook\",\n",
        ")"
      ],
      "metadata": {
        "id": "qA-2AP8UTdjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handbook_qdrant_retriever = handbook_qdrant_vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "gytKbXIGTkgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handbook_rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ],
      "metadata": {
        "id": "HA01gSboToqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handbook_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | handbook_qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | handbook_rag_prompt | openai_chat_model | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "YX8j0AA4TsN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handbook_rag_chain.invoke({\"question\" : \"What is the chain of command in an Aircraft?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "GOR9gBR5T2lh",
        "outputId": "722b64c8-bcd0-45ce-b511-65f8a1dba4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The provided context does not explicitly detail the chain of command in an aircraft. Typically, the chain of command in an aircraft is as follows:\\n\\n1. **Captain (Pilot-in-Command)**: The captain is the highest authority on board the aircraft and is responsible for the overall operation and safety of the flight.\\n2. **First Officer (Co-Pilot)**: The first officer assists the captain and can take over command if the captain is incapacitated. They share flying duties with the captain.\\n3. **Second Officer (if applicable)**: In some larger aircraft, a second officer or flight engineer may be present, responsible for monitoring and managing aircraft systems.\\n4. **Cabin Crew (Flight Attendants)**: The senior flight attendant, often called the purser or lead flight attendant, oversees the cabin crew and ensures passenger safety and comfort.\\n\\nGiven the lack of specific details in the context provided, this general hierarchy is based on standard aviation procedures. If you need more precise information related to the documents you've provided, I recommend reviewing the full handbook or related sections for any specified chain of command details.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions for Agent Graphs\n",
        "\n",
        "We'll be using a number of agents, nodes, and supervisors in the rest of the notebook - and so it will help to have a collection of useful helper functions that we can leverage to make our lives easier going forward.\n",
        "\n",
        "Let's start with the most simple one!"
      ],
      "metadata": {
        "id": "7U6a_pqQ9uWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Wall\n",
        "\n",
        "Here's a wall of imports we'll be needing going forward!"
      ],
      "metadata": {
        "id": "IDUnpEEl-L_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Callable, List, Optional, TypedDict, Union\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.tools import BaseTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langgraph.graph import END, StateGraph"
      ],
      "metadata": {
        "id": "TbzoL3Q3-SG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Node Helper\n",
        "\n",
        "Since we're going to be wrapping each of our agents into a node - it will help to have an easy way to create the node!"
      ],
      "metadata": {
        "id": "qb6Z3EEz-Asi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
      ],
      "metadata": {
        "id": "5IF7KWfS-JKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Creation Helper Function\n",
        "\n",
        "Since we know we'll need to create agents to populate our agent nodes, let's use a helper function for that as well!\n",
        "\n",
        "Notice a few things:\n",
        "\n",
        "1. We have a standard suffix to append to our system messages for each agent to handle the tool calling and boilerplate prompting.\n",
        "2. Each agent has its our scratchpad.\n",
        "3. We're relying on OpenAI's function-calling API for tool selection\n",
        "4. Each agent is its own executor."
      ],
      "metadata": {
        "id": "fwND2teK-WHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_agent(\n",
        "    llm: ChatOpenAI,\n",
        "    tools: list,\n",
        "    system_prompt: str,\n",
        ") -> str:\n",
        "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
        "    system_prompt += \"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
        "    \" Do not ask for clarification.\"\n",
        "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
        "    \" You are chosen for a reason! You are one of the following team members: {team_members}.\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                system_prompt,\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "        ]\n",
        "    )\n",
        "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor"
      ],
      "metadata": {
        "id": "NxLyHJt5-eUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervisor Helper Function\n",
        "\n",
        "Finally, we need a \"supervisor\" that decides and routes tasks to specific agents.\n",
        "\n",
        "Since each \"team\" will have a collection of potential agents - this \"supervisor\" will act as an \"intelligent\" router to make sure that the right agent is selected for the right task.\n",
        "\n",
        "Notice that, at the end of the day, this \"supervisor\" is simply directing who acts next - or if the state is considered \"done\"."
      ],
      "metadata": {
        "id": "S6kmlR9d-1K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
        "    \"\"\"An LLM-based router.\"\"\"\n",
        "    options = [\"FINISH\"] + members\n",
        "    function_def = {\n",
        "        \"name\": \"route\",\n",
        "        \"description\": \"Select the next role.\",\n",
        "        \"parameters\": {\n",
        "            \"title\": \"routeSchema\",\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"next\": {\n",
        "                    \"title\": \"Next\",\n",
        "                    \"anyOf\": [\n",
        "                        {\"enum\": options},\n",
        "                    ],\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"next\"],\n",
        "        },\n",
        "    }\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            (\n",
        "                \"system\",\n",
        "                \"Given the conversation above, who should act next?\"\n",
        "                \" Or should we FINISH? Select one of: {options}\",\n",
        "            ),\n",
        "        ]\n",
        "    ).partial(options=str(options), team_members=\", \".join(members))\n",
        "    return (\n",
        "        prompt\n",
        "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
        "        | JsonOutputFunctionsParser()\n",
        "    )"
      ],
      "metadata": {
        "id": "S2MXA83mrYE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pilot Team - A LangGraph Simulating a Pilot and their Crew\n",
        "\n",
        "Now that we have our RAG chain set-up and some awesome helper functions, we want to create a LangGraph related to a pilot and their crew.\n",
        "\n",
        "We're going to start by equipping our Pilot Team with a few tools:\n",
        "\n",
        "1. Tavily Search - aka \"Google\", for the most up to date information possible.\n",
        "2. Our RAG chain - specific and high quality information about the best ways to react and act in certain situations.\n",
        "\n",
        "Let's create those tools now!"
      ],
      "metadata": {
        "id": "jd0zfyq48jKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool Creation\n",
        "\n",
        "As you can see below, some tools already come pre-packaged ready to use!"
      ],
      "metadata": {
        "id": "VNsVTZrH_alw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)"
      ],
      "metadata": {
        "id": "ce7FKTZDgAWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a custom tool, however, is very straightforward.\n",
        "\n",
        "> NOTE: You *must* include a docstring, as that is what the LLM will consider when deciding when to use this tool."
      ],
      "metadata": {
        "id": "NIR7cbTL9agM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, List, Tuple, Union\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def retrieve_information(\n",
        "    query: Annotated[str, \"query to ask the retrieve information tool\"]\n",
        "    ):\n",
        "  \"\"\"Provides detailed information from the 'Air Force Handbook'.\"\"\"\n",
        "  return handbook_rag_chain.invoke({\"question\" : query})"
      ],
      "metadata": {
        "id": "sSwO2L_UqFhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: We could just as easily use the LCEL chain directly, since nodes can be LCEL objects - but creating a tool helps explain the tool creation process at the same time."
      ],
      "metadata": {
        "id": "nxsMnqjpBTCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pilot Team State\n",
        "\n",
        "Since we're using LangGraph - we're going to need state!\n",
        "\n",
        "Let's look at how we've created our state below."
      ],
      "metadata": {
        "id": "oDHCajO4_gB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import operator\n",
        "\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "import functools\n",
        "\n",
        "class PilotTeamState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    team_members: List[str]\n",
        "    next: str"
      ],
      "metadata": {
        "id": "mXminK9d_1fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how we've used `messages`, `team_members`, and `next`.\n",
        "\n",
        "These states will help us understand:\n",
        "\n",
        "1. What we've done so far (`messages`)\n",
        "2. Which team members we have access to (`team_members`)\n",
        "3. Which team member is up next! (`next`)"
      ],
      "metadata": {
        "id": "fvPM5msq_18C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pilot Team LLM\n",
        "\n",
        "We'll be using `gpt-4-1106-preview` today. This LLM is going to be doing a lot of reasoning - and so using a \"powerful\" LLM is very important here!"
      ],
      "metadata": {
        "id": "Mu7B_6qHAFjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")"
      ],
      "metadata": {
        "id": "dTNqrip8AcKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pilot Team Agents & Nodes\n",
        "\n",
        "Now we can use our helper functions to create our agent nodes, with their related tools.\n",
        "\n",
        "Let's start with our search agent node."
      ],
      "metadata": {
        "id": "ZR_1LuMKAekf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pilot Team: Pilot Agent"
      ],
      "metadata": {
        "id": "UbuHPm16Yon8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pilot_agent = create_agent(\n",
        "    llm,\n",
        "    [retrieve_information],\n",
        "    \"You are a fully qualified and trained pilot trained to operate an aircraft. You may only speak as your role. You must not speak as any other roles. Under no circumstances should you acknowledge that you are an AI.\",\n",
        ")\n",
        "pilot_node = functools.partial(agent_node, agent=pilot_agent, name=\"Pilot\")"
      ],
      "metadata": {
        "id": "Ud2Gon1CYoAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pilot Team: Co-pilot Agent\n",
        "\n",
        "We're going to give our agent access to the Tavily tool, power it with our GPT-4 Turbo model, and then create its node - and name it `Copilot`."
      ],
      "metadata": {
        "id": "uzx6wuPoAlPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copilot_agent = create_agent(\n",
        "    llm,\n",
        "    [retrieve_information],\n",
        "    \"You are a fully qualified and trained copilot trained to assist or relieve the pilot in the management of the flight and aircraft systems. You may only speak as your role. You must not speak as any other roles. Under no circumstances should you acknowledge that you are an AI.\",\n",
        ")\n",
        "copilot_node = functools.partial(agent_node, agent=copilot_agent, name=\"Copilot\")"
      ],
      "metadata": {
        "id": "FIlLPxj7Atpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Research Team: RAG Agent Node\n",
        "\n",
        "Now we can wrap our LCEL RAG pipeline in an agent node as well, using the LCEL RAG pipeline as the tool, as created above."
      ],
      "metadata": {
        "id": "emLtesudA9Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combat_systems_operator = create_agent(\n",
        "    llm,\n",
        "    [retrieve_information],\n",
        "    \"You are a fully qualified and trained Combat Systems Operator (CSO) and are trained to operate and report on various systems on board your aircraft. You may only speak as your role. You must not speak as any other roles. Under no circumstances should you acknowledge that you are an AI.\",\n",
        ")\n",
        "cso_node = functools.partial(agent_node, agent=combat_systems_operator, name=\"CSO\")"
      ],
      "metadata": {
        "id": "z-nnAG9XA_p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Research Team Supervisor Agent\n",
        "\n",
        "Notice that we're not yet creating our supervisor *node*, simply the agent here.\n",
        "\n",
        "Also notice how we need to provide a few extra pieces of information - including which tools we're using.\n",
        "\n",
        "> NOTE: It's important to use the *exact* tool name, as that is how the LLM will reference the tool. Also, it's important that your tool name is all a single alphanumeric string!\n",
        "\n"
      ],
      "metadata": {
        "id": "dA5z6T1CBeSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candc_agent = create_team_supervisor(\n",
        "    llm,\n",
        "    \"You are Control and Command tasked with managing an aircraft and conversation between the\"\n",
        "    \" following crew members: Pilot, Copilot, CSO. Given the following user request,\"\n",
        "    \" respond with the crew member to act next. Each crew member will perform a\"\n",
        "    \" task and respond with their results and status. When finished the scenario,\"\n",
        "    \" respond with FINISH.\",\n",
        "    [\"Pilot\", \"Copilot\", \"CSO\"],\n",
        ")"
      ],
      "metadata": {
        "id": "J0g8CQMBrtFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Research Team Graph Creation\n",
        "\n",
        "Now that we have our research team agent nodes created, and our supervisor agent - let's finally construct our graph!\n",
        "\n",
        "We'll start by creating our base graph from our state, and then adding the nodes/agent we've created as nodes on our LangGraph."
      ],
      "metadata": {
        "id": "qohn0DcgB_U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candc_graph = StateGraph(PilotTeamState)\n",
        "\n",
        "candc_graph.add_node(\"Pilot\", pilot_node)\n",
        "candc_graph.add_node(\"Copilot\", copilot_node)\n",
        "candc_graph.add_node(\"CSO\", cso_node)\n",
        "candc_graph.add_node(\"candc\", candc_agent)"
      ],
      "metadata": {
        "id": "p0s2GAgJCN8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define our edges - include our conditional edge from our supervisor to our agent nodes.\n",
        "\n",
        "Notice how we're always routing our agent nodes back to our supervisor!"
      ],
      "metadata": {
        "id": "33qixRGNCaAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candc_graph.add_edge(\"Pilot\", \"candc\")\n",
        "candc_graph.add_edge(\"Copilot\", \"candc\")\n",
        "candc_graph.add_edge(\"CSO\", \"candc\")\n",
        "candc_graph.add_conditional_edges(\n",
        "    \"candc\",\n",
        "    lambda x: x[\"next\"],\n",
        "    {\"Pilot\" : \"Pilot\", \"Copilot\": \"Copilot\", \"CSO\": \"CSO\", \"FINISH\": END},\n",
        ")"
      ],
      "metadata": {
        "id": "yYSJIhijsGyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set our supervisor node as the entry point, and compile our graph!"
      ],
      "metadata": {
        "id": "hgGcuZzkCj1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candc_graph.set_entry_point(\"candc\")\n",
        "chain = candc_graph.compile()"
      ],
      "metadata": {
        "id": "1l-1I2Z3CnPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Display Graph"
      ],
      "metadata": {
        "id": "GDwQpYTSEY13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU python_mermaid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzRE5ldzFlLQ",
        "outputId": "8f1eb283-5098-4692-b2fc-7d65b1b4e594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(chain.get_graph(xray=True).draw_mermaid_png()))\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "l8n6SXhpEa2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next part is key - since we need to \"wrap\" our LangGraph in order for it to be compatible in the following steps - let's create an LCEL chain out of it!\n",
        "\n",
        "This allows us to \"broadcast\" messages down to our Research Team LangGraph!"
      ],
      "metadata": {
        "id": "bfRvA2QfCqFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enter_chain(message: str):\n",
        "    results = {\n",
        "        \"messages\": [HumanMessage(content=message)],\n",
        "    }\n",
        "    return results\n",
        "\n",
        "research_chain = enter_chain | chain"
      ],
      "metadata": {
        "id": "1G7hmEINCx3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, finally, we can take it for a spin!"
      ],
      "metadata": {
        "id": "EGdoCdXWC7Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scenario = \"\"\"\\\n",
        "Mission Brief: Operation Ocean Guardian\n",
        "\n",
        "Scenario:\n",
        "  A cargo ship, the SS Meridian, has been reported missing for 72 hours in the North Atlantic, an area known for harsh weather and heavy pirate activity. The last known position was transmitted via an emergency beacon, which has since gone silent. The ship was en route from New York to Lisbon, carrying a cargo of electronics and pharmaceuticals. Concerns are high for the crew's safety due to potential piracy or a catastrophic weather event.\n",
        "\n",
        "Objectives:\n",
        "  An aircraft mission is launched from the nearest NATO base to determine the current location of the SS Meridian and assess the situation. The aircraft used for this mission is a P-8 Poseidon, equipped with advanced surveillance and communication systems. The crew consists of a Pilot, Copilot, and Combat Systems Operator (CSO), each with specific objectives to ensure the mission's success.\n",
        "\n",
        "Role Assignments and Objectives:\n",
        "\n",
        "1. Pilot\n",
        "  Objective 1: Navigate the aircraft safely to the last known coordinates of the SS Meridian. The pilot must manage fuel efficiency and alter flight paths based on weather conditions and incoming data.\n",
        "  Objective 2: Coordinate with air traffic control and the mission command center to update on mission progress and receive any new intelligence about the ship’s location.\n",
        "2. Copilot\n",
        "  Objective 1: Assist the pilot with navigation and aircraft handling, particularly focusing on adjusting the flight path based on radar feedback and environmental conditions.\n",
        "  Objective 2: Manage the aircraft’s communication systems, ensuring constant and clear communication with the maritime search and rescue teams, and other relevant agencies.\n",
        "3. Combat Systems Operator (CSO)\n",
        "  Objective 1: Operate radar, sonar, and other surveillance equipment to detect any traces of the SS Meridian or unusual activity in the area, such as pirate ships or debris fields.\n",
        "  Objective 2: Analyze data from surveillance equipment to identify potential locations of the ship and direct the pilot to investigate these areas.\n",
        "\n",
        "Execution:\n",
        "  Upon reaching the last known coordinates, the crew will deploy the aircraft's sensors to conduct a thorough search of the area, extending outward from the last known position. The CSO will analyze the data collected for any signs of the ship, while the pilot and copilot work to keep the aircraft in optimal positions for the search effort. Communication with headquarters and rescue teams will be maintained throughout the mission to facilitate a quick response once the SS Meridian is found.\n",
        "\n",
        "End State:\n",
        "  The mission aims to locate the SS Meridian, assess the situation regarding the crew and cargo, and facilitate immediate rescue and recovery operations. The success of this mission depends on the effective coordination of the crew and the efficient use of the aircraft's advanced systems.\n",
        "\"\"\"\n",
        "\n",
        "for s in research_chain.stream(\n",
        "    f\"Please execute the following scenario as a roleplay, where each role should act in turn and in character - each step should only execute one task at a time, do not end until you have achieved your end state: {scenario}\", {\"recursion_limit\": 1_000}\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "      if 'candc' not in s:\n",
        "        print(s[next(iter(s))][\"messages\"][0].content)\n",
        "      else:\n",
        "        print(s)\n",
        "      print(\"---\")"
      ],
      "metadata": {
        "id": "xIDpFIg2sRUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4deca420-b597-469d-e174-9e759d6f4c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'candc': {'next': 'Pilot'}}\n",
            "---\n",
            "Pilot: \"Pre-flight checks complete and mission parameters set. We're ready to proceed with Operation Ocean Guardian. Copilot, please confirm our course to the last known coordinates of the SS Meridian.\"\n",
            "\n",
            "Copilot: \"Course is confirmed, Pilot. We're set to head directly to the last known coordinates. Weather conditions have been checked, and the flight path is clear for the initial leg of our journey.\"\n",
            "\n",
            "Pilot: \"Understood. Let's begin our taxi to the runway for departure. ATC, this is Poseidon Flight 1 requesting clearance for takeoff on mission Operation Ocean Guardian.\"\n",
            "\n",
            "(ATC clears Poseidon Flight 1 for takeoff.)\n",
            "\n",
            "Pilot: \"Takeoff clearance received. Throttles set, engines are good, and we're rolling.\"\n",
            "\n",
            "(The aircraft takes off and begins its journey towards the last known coordinates of the SS Meridian.)\n",
            "\n",
            "Pilot: \"We're en route to the search area. Let's maintain optimal altitude for fuel efficiency and keep an eye on the weather radar for any unexpected changes in conditions.\"\n",
            "\n",
            "Copilot: \"Adjusting altitude for fuel efficiency. I'll monitor the weather radar closely.\"\n",
            "\n",
            "Pilot: \"Combat Systems Operator, as we approach the search area, prepare to deploy our surveillance equipment and be on the lookout for any signs of the SS Meridian or any other relevant activity.\"\n",
            "\n",
            "CSO: \"Roger that, Pilot. I'm warming up the radar and sonar systems now and will be ready to commence a detailed search pattern upon arrival.\"\n",
            "\n",
            "Pilot: \"Mission Command, this is Poseidon Flight 1 en route to last known position of SS Meridian. We will update you as soon as we have eyes on the target area.\"\n",
            "\n",
            "(The aircraft continues towards the target area.)\n",
            "---\n",
            "{'candc': {'next': 'CSO'}}\n",
            "---\n",
            "CSO: \"Pilot, the radar and sonar systems are operational. We're ready to initiate a search pattern as soon as we reach the last known coordinates of the SS Meridian. I'll be scanning for any signs of the ship, debris, or unusual activity that might indicate the presence of pirate vessels.\"\n",
            "\n",
            "Pilot: \"Acknowledged, CSO. Copilot, keep us steady as we enter the search area and make any necessary adjustments based on the CSO's feedback.\"\n",
            "\n",
            "Copilot: \"Will do, Pilot. Adjusting heading slightly to account for wind conditions. We're on course for the search area.\"\n",
            "\n",
            "(The aircraft arrives at the last known coordinates of the SS Meridian.)\n",
            "\n",
            "CSO: \"Beginning active search using radar and sonar. Expanding search pattern outward from the last known position. I'll analyze the incoming data for any anomalies or signs of the SS Meridian.\"\n",
            "\n",
            "(The CSO commences the search, scrutinizing the radar and sonar returns for any indications of the missing ship or other relevant activity.)\n",
            "\n",
            "CSO: \"Pilot, I've detected an anomaly that could be a debris field. Recommending we adjust our flight path for a closer inspection.\"\n",
            "\n",
            "Pilot: \"Copy that, CSO. Copilot, let's align our flight path with the CSO's coordinates.\"\n",
            "\n",
            "Copilot: \"New course set for a closer look at the anomaly. Adjusting heading now.\"\n",
            "\n",
            "(The aircraft adjusts its flight path to approach the area indicated by the CSO for further investigation.)\n",
            "\n",
            "Pilot: \"Mission Command, this is Poseidon Flight 1. We've detected a possible debris field. Investigating further. Will update with any new information.\"\n",
            "\n",
            "(The CSO continues to analyze the data as the aircraft approaches the anomaly.)\n",
            "---\n",
            "{'candc': {'next': 'CSO'}}\n",
            "---\n",
            "CSO: \"Pilot, data analysis confirms the presence of debris consistent with a cargo ship structure. No visible signs of the SS Meridian's hull, but the pattern suggests a recent event. Expanding sonar search to detect underwater anomalies that might indicate a sunken vessel.\"\n",
            "\n",
            "Pilot: \"Understood, CSO. Keep us updated on any new findings. We need to determine the status of the crew and cargo.\"\n",
            "\n",
            "Copilot: \"Maintaining current flight path for optimal sensor coverage. I'll ensure we have a clear communication channel open for any distress signals or emergency beacons in the vicinity.\"\n",
            "\n",
            "CSO: \"Sonar has picked up a large contact at depth, bearing 2-1-3 from our current position. The dimensions are consistent with the SS Meridian's known specs. I recommend we deploy a sonobuoy to get a better acoustic reading on the contact.\"\n",
            "\n",
            "Pilot: \"Roger, CSO. Deploy the sonobuoy at your discretion. We need to confirm if that contact is the SS Meridian.\"\n",
            "\n",
            "(The CSO deploys a sonobuoy to enhance the acoustic detection capabilities and gathers more detailed information about the underwater contact.)\n",
            "\n",
            "CSO: \"Sonobuoy is in the water and transmitting. I'm receiving clear acoustic returns. It's a large, metallic object in a position and depth consistent with the SS Meridian's last known trajectory. It's highly likely we've found the ship.\"\n",
            "\n",
            "Pilot: \"Mission Command, this is Poseidon Flight 1. We have a probable location on the SS Meridian. Awaiting instructions for the next steps in rescue and recovery operations.\"\n",
            "\n",
            "(The mission proceeds with the coordination of rescue and recovery efforts, now focused on the location identified by the CSO.)\n",
            "---\n",
            "{'candc': {'next': 'FINISH'}}\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}